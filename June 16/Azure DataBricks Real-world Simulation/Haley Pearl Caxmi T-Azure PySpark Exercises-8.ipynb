{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40662618-1da6-4b55-b39b-276f3a6107c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-237925853105861916\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b1560985490>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bbe20b-e9f1-4d5b-a2ed-eed7fbbe0918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+----------+----------+--------+--------+---------+\n|SubscriptionID|UserID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n|        SUB001|  U001|   Basic|2024-01-01|2024-04-01|    30.0|    true|     true|\n|        SUB002|  U002|     Pro|2024-02-15|2024-05-15|    90.0|    true|    false|\n|        SUB003|  U003|     Pro|2024-03-10|2024-06-10|    90.0|   false|    false|\n|        SUB004|  U001| Premium|2024-04-05|2024-07-05|   120.0|    true|     true|\n|        SUB005|  U004|   Basic|2024-01-20|2024-04-20|    30.0|   false|    false|\n|        SUB006|  U005|   Basic|2024-01-05|2024-02-05|    30.0|    true|     true|\n|        SUB007|  U005|     Pro|2024-02-06|2024-03-06|    90.0|    true|     true|\n|        SUB008|  U005| Premium|2024-03-07|2024-04-07|   120.0|    true|     true|\n|        SUB009|  U006|   Basic|2024-01-10|2024-04-10|    30.0|    true|    false|\n|        SUB010|  U006|     Pro|2024-04-11|2024-05-11|    90.0|    true|    false|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n\n+------+-------------------+---------+-----------+\n|UserID|          EventTime|EventType|FeatureUsed|\n+------+-------------------+---------+-----------+\n|  U001|2024-04-07 10:22:00|    login|  Dashboard|\n|  U002|2024-04-08 11:10:00|   upload|    Reports|\n|  U003|2024-04-09 09:45:00| download|  Analytics|\n|  U001|2024-04-10 16:00:00|   logout|  Dashboard|\n|  U004|2024-04-11 12:00:00|    login|  Dashboard|\n|  U005|2024-03-01 10:00:00|    login|  Dashboard|\n|  U005|2024-03-02 10:00:00|   upload|    Reports|\n|  U005|2024-03-03 10:00:00|    login|  Analytics|\n|  U005|2024-03-04 10:00:00|    login|    Reports|\n|  U006|2024-03-01 11:00:00|    login|  Dashboard|\n|  U006|2024-03-02 11:30:00|   upload|  Dashboard|\n|  U006|2024-03-03 12:00:00|    login|  Analytics|\n+------+-------------------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Load both datasets with schema inference\n",
    "df_subs = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file:/Workspace/Shared/subscriptions.csv\")\n",
    "\n",
    "df_activity = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file:/Workspace/Shared/user_activity.csv\")\n",
    "\n",
    "df_subs.show()\n",
    "df_activity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28906ab8-d12f-4208-8196-98e9208c30d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------+----------+----------+--------+--------+---------+-----------+---------------+------------------+\n|UserID|SubscriptionID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|active_days|events_per_user|  engagement_score|\n+------+--------------+--------+----------+----------+--------+--------+---------+-----------+---------------+------------------+\n|  U001|        SUB001|   Basic|2024-01-01|2024-04-01|    30.0|    true|     true|         91|              2|0.6593406593406594|\n|  U002|        SUB002|     Pro|2024-02-15|2024-05-15|    90.0|    true|    false|         90|              1|               1.0|\n|  U003|        SUB003|     Pro|2024-03-10|2024-06-10|    90.0|   false|    false|         92|              1|0.9782608695652174|\n|  U001|        SUB004| Premium|2024-04-05|2024-07-05|   120.0|    true|     true|         91|              2|2.6373626373626378|\n|  U004|        SUB005|   Basic|2024-01-20|2024-04-20|    30.0|   false|    false|         91|              1|0.3296703296703297|\n|  U005|        SUB006|   Basic|2024-01-05|2024-02-05|    30.0|    true|     true|         31|              4| 3.870967741935484|\n|  U005|        SUB007|     Pro|2024-02-06|2024-03-06|    90.0|    true|     true|         29|              4|12.413793103448276|\n|  U005|        SUB008| Premium|2024-03-07|2024-04-07|   120.0|    true|     true|         31|              4|15.483870967741936|\n|  U006|        SUB009|   Basic|2024-01-10|2024-04-10|    30.0|    true|    false|         91|              3| 0.989010989010989|\n|  U006|        SUB010|     Pro|2024-04-11|2024-05-11|    90.0|    true|    false|         30|              3|               9.0|\n+------+--------------+--------+----------+----------+--------+--------+---------+-----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#A â€“ Subscription Engagement Score\n",
    "from pyspark.sql.functions import col, count, datediff\n",
    "\n",
    "df_subs_engaged = df_subs.withColumn(\"active_days\", datediff(\"EndDate\", \"StartDate\"))\n",
    "\n",
    "df_events_per_user = df_activity.groupBy(\"UserID\") \\\n",
    "    .agg(count(\"EventType\").alias(\"events_per_user\"))\n",
    "\n",
    "df_engagement = df_subs_engaged.join(df_events_per_user, \"UserID\", \"left\") \\\n",
    "    .withColumn(\"engagement_score\", (col(\"events_per_user\") / col(\"active_days\")) * col(\"PriceUSD\"))\n",
    "\n",
    "df_engagement.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9394f134-0857-4697-aafa-43a49da9ce7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n|UserID|         AnomalyType|\n+------+--------------------+\n|  U004|   InactiveButActive|\n|  U003|   InactiveButActive|\n|  U001|AutoRenewNoActivi...|\n|  U005|AutoRenewNoActivi...|\n+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# B - Anomaly Detection via SQL\n",
    "#Users who have inactive subscriptions but recent activity\n",
    "#Users who have AutoRenew = true but no activity in last 30 days\n",
    "df_subs.createOrReplaceTempView(\"subscriptions\")\n",
    "df_activity.createOrReplaceTempView(\"user_activity\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW anomaly_users_combined AS\n",
    "(\n",
    "    SELECT DISTINCT s.UserID, 'InactiveButActive' AS AnomalyType\n",
    "    FROM subscriptions s\n",
    "    JOIN user_activity u ON s.UserID = u.UserID\n",
    "    WHERE s.IsActive = false\n",
    "\n",
    "    UNION\n",
    "\n",
    "    SELECT s.UserID, 'AutoRenewNoActivity30Days' AS AnomalyType\n",
    "    FROM subscriptions s\n",
    "    LEFT JOIN user_activity u ON s.UserID = u.UserID\n",
    "    WHERE s.AutoRenew = true\n",
    "    GROUP BY s.UserID\n",
    "    HAVING MAX(to_timestamp(u.EventTime)) < current_timestamp() - INTERVAL 30 DAYS\n",
    "           OR MAX(to_timestamp(u.EventTime)) IS NULL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# View anomalies\n",
    "spark.sql(\"SELECT * FROM anomaly_users_combined\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc021e7f-3521-4d10-868f-5c847304ef1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+----------+----------+--------+--------+---------+\n|SubscriptionID|UserID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n|        SUB003|  U003|     Pro|2024-03-10|2024-06-10|    95.0|   false|    false|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#C â€“ Delta Lake Merge Simulation (Pro Plan Price Fix)\n",
    "from pyspark.sql.functions import month, lit\n",
    "\n",
    "# Save to Delta\n",
    "df_subs.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .save(\"/Workspace/Shared/delta_subscriptions\")\n",
    "\n",
    "# Load from Delta\n",
    "df_delta = spark.read.format(\"delta\").load(\"/Workspace/Shared/delta_subscriptions\")\n",
    "df_delta.createOrReplaceTempView(\"delta_subscriptions\")\n",
    "\n",
    "# Create fix DataFrame\n",
    "df_fix = df_delta.filter((col(\"PlanType\") == \"Pro\") & (month(\"StartDate\") == 3)) \\\n",
    "    .withColumn(\"PriceUSD\", col(\"PriceUSD\") + lit(5))\n",
    "\n",
    "df_fix.createOrReplaceTempView(\"fix_table\")\n",
    "df_fix.show()\n",
    "# Apply merge\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO delta_subscriptions AS main\n",
    "USING fix_table AS fix\n",
    "ON main.SubscriptionID = fix.SubscriptionID\n",
    "WHEN MATCHED THEN UPDATE SET main.PriceUSD = fix.PriceUSD\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d196ad-1f23-42a9-a8a5-bfe8c9194fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|operation| operationParameters| job|         notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      7|2025-06-16 10:49:08|8778822765517627|azuser3551_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{848225530731555}|0611-043435-vg20yowf|          6|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      6|2025-06-16 10:49:04|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{848225530731555}|0611-043435-vg20yowf|          5|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      5|2025-06-16 10:44:09|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{848225530731555}|0611-043435-vg20yowf|          4|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      4|2025-06-16 10:44:07|8778822765517627|azuser3551_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{848225530731555}|0611-043435-vg20yowf|          3|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      3|2025-06-16 10:44:02|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{848225530731555}|0611-043435-vg20yowf|          2|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      2|2025-06-16 10:40:44|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{848225530731555}|0611-043435-vg20yowf|          1|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      1|2025-06-16 10:40:40|8778822765517627|azuser3551_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{848225530731555}|0611-043435-vg20yowf|          0|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      0|2025-06-16 10:40:31|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{848225530731555}|0611-043435-vg20yowf|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n+--------------+------+--------+----------+----------+--------+--------+---------+\n|SubscriptionID|UserID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n|        SUB001|  U001|   Basic|2024-01-01|2024-04-01|    30.0|    true|     true|\n|        SUB002|  U002|     Pro|2024-02-15|2024-05-15|    90.0|    true|    false|\n|        SUB003|  U003|     Pro|2024-03-10|2024-06-10|    90.0|   false|    false|\n|        SUB004|  U001| Premium|2024-04-05|2024-07-05|   120.0|    true|     true|\n|        SUB005|  U004|   Basic|2024-01-20|2024-04-20|    30.0|   false|    false|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#D â€“ Time Travel Debugging\n",
    "# View Delta version history\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/Workspace/Shared/delta_subscriptions`\").show()\n",
    "\n",
    "# Read before merge using version\n",
    "df_old = spark.read.format(\"delta\").option(\"versionAsOf\", 0) \\\n",
    "    .load(\"/Workspace/Shared/delta_subscriptions\")\n",
    "\n",
    "df_old.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3e6506-a6f3-41d3-b270-ab91bee46a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+----------+\n|UserID|prev_plan|PlanType| StartDate|\n+------+---------+--------+----------+\n|  U005|    Basic|     Pro|2024-02-06|\n|  U005|      Pro| Premium|2024-03-07|\n|  U006|    Basic|     Pro|2024-04-11|\n+------+---------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#E â€“ Tier Migration (Basic â†’ Pro â†’ Premium)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "win = Window.partitionBy(\"UserID\").orderBy(\"StartDate\")\n",
    "\n",
    "# Detect plan migration\n",
    "df_migration = df_subs.withColumn(\"prev_plan\", lag(\"PlanType\").over(win))\n",
    "\n",
    "# Filter: Basic to Pro, or Pro to Premium\n",
    "df_migration.filter(\n",
    "    ((col(\"prev_plan\") == \"Basic\") & (col(\"PlanType\") == \"Pro\")) |\n",
    "    ((col(\"prev_plan\") == \"Pro\") & (col(\"PlanType\") == \"Premium\"))\n",
    ").select(\"UserID\", \"prev_plan\", \"PlanType\", \"StartDate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0064782d-bcd5-4509-8dae-e3cf06380a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+\n|UserID|features_used|login_count|\n+------+-------------+-----------+\n|  U005|            3|          3|\n+------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#F â€“ Power Users Detection\n",
    "from pyspark.sql.functions import countDistinct, sum\n",
    "\n",
    "# Power user = Used â‰¥2 features and logged in â‰¥3 times\n",
    "df_power_users = df_activity.groupBy(\"UserID\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"FeatureUsed\").alias(\"features_used\"),\n",
    "        sum((col(\"EventType\") == \"login\").cast(\"int\")).alias(\"login_count\")\n",
    "    ).filter((col(\"features_used\") >= 2) & (col(\"login_count\") >= 3))\n",
    "\n",
    "# Save to Delta table\n",
    "df_power_users.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .save(\"/Workspace/Shared/power_users\")\n",
    "\n",
    "df_power_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff65f299-ee49-46c5-8a35-2b722508bb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------------+------------+\n|UserID|prev_event|          EventTime|session_secs|\n+------+----------+-------------------+------------+\n|  U001|     login|2024-04-10 16:00:00|      279480|\n+------+----------+-------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#G â€“ Session Replay View (Session Duration)\n",
    "\n",
    "from pyspark.sql.functions import when,unix_timestamp\n",
    "\n",
    "window_spec = Window.partitionBy(\"UserID\").orderBy(\"EventTime\")\n",
    "\n",
    "df_session = df_activity.withColumn(\"event_ts\", unix_timestamp(\"EventTime\")) \\\n",
    "    .withColumn(\"prev_ts\", lag(\"event_ts\").over(window_spec)) \\\n",
    "    .withColumn(\"prev_event\", lag(\"EventType\").over(window_spec)) \\\n",
    "    .withColumn(\"session_secs\", \n",
    "                when(col(\"EventType\") == \"logout\", col(\"event_ts\") - col(\"prev_ts\"))) \\\n",
    "    .filter(col(\"EventType\") == \"logout\")\n",
    "\n",
    "df_session.select(\"UserID\", \"prev_event\", \"EventTime\", \"session_secs\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Azure PySpark Exercises-8",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}