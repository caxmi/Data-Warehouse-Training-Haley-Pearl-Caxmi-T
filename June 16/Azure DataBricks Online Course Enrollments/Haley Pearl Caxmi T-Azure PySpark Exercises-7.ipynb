{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5ab2b4-4af3-4a85-a60a-aa1ecae22cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-5889765894395603884\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7e8b445e8710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c692e0f-95be-4b43-98f7-68d5d95497bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------------+-----------+----------+---------------+------+---------+\n|EnrollmentID|StudentName|          CourseName|   Category|EnrollDate|ProgressPercent|Rating|   Status|\n+------------+-----------+--------------------+-----------+----------+---------------+------+---------+\n|      ENR001|     Aditya|Python for Beginners|Programming|2024-05-10|             80|   4.5|   Active|\n|      ENR002|     Simran|Data Analysis wit...|  Analytics|2024-05-12|            100|   4.7|Completed|\n|      ENR003|     Aakash| Power BI Essentials|  Analytics|2024-05-13|             30|   3.8|   Active|\n|      ENR004|       Neha|         Java Basics|Programming|2024-05-15|              0|  NULL| Inactive|\n|      ENR005|       Zara|Machine Learning 101|         AI|2024-05-17|             60|   4.2|   Active|\n|      ENR006|    Ibrahim|Python for Beginners|Programming|2024-05-18|             90|   4.6|Completed|\n+------------+-----------+--------------------+-----------+----------+---------------+------+---------+\n\nroot\n |-- EnrollmentID: string (nullable = true)\n |-- StudentName: string (nullable = true)\n |-- CourseName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- EnrollDate: date (nullable = true)\n |-- ProgressPercent: integer (nullable = true)\n |-- Rating: double (nullable = true)\n |-- Status: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#1. Load the data with schema inference enabled\n",
    "df_infer = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file:/Workspace/Shared/course_enrollments.csv\")\n",
    "\n",
    "df_infer.show()\n",
    "df_infer.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd594896-c449-41a1-a242-b0dc9f2be95d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------------+-----------+----------+---------------+------+---------+\n|EnrollmentID|StudentName|          CourseName|   Category|EnrollDate|ProgressPercent|Rating|   Status|\n+------------+-----------+--------------------+-----------+----------+---------------+------+---------+\n|      ENR001|     Aditya|Python for Beginners|Programming|2024-05-10|             80|   4.5|   Active|\n|      ENR002|     Simran|Data Analysis wit...|  Analytics|2024-05-12|            100|   4.7|Completed|\n|      ENR003|     Aakash| Power BI Essentials|  Analytics|2024-05-13|             30|   3.8|   Active|\n|      ENR004|       Neha|         Java Basics|Programming|2024-05-15|              0|  NULL| Inactive|\n|      ENR005|       Zara|Machine Learning 101|         AI|2024-05-17|             60|   4.2|   Active|\n|      ENR006|    Ibrahim|Python for Beginners|Programming|2024-05-18|             90|   4.6|Completed|\n+------------+-----------+--------------------+-----------+----------+---------------+------+---------+\n\nroot\n |-- EnrollmentID: string (nullable = true)\n |-- StudentName: string (nullable = true)\n |-- CourseName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- EnrollDate: date (nullable = true)\n |-- ProgressPercent: integer (nullable = true)\n |-- Rating: double (nullable = true)\n |-- Status: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#2. Manually define schema and compare both approaches\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "manual_schema = StructType([\n",
    "    StructField(\"EnrollmentID\", StringType(), True),\n",
    "    StructField(\"StudentName\", StringType(), True),\n",
    "    StructField(\"CourseName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"EnrollDate\", DateType(), True),\n",
    "    StructField(\"ProgressPercent\", IntegerType(), True),\n",
    "    StructField(\"Rating\", DoubleType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_manual = spark.read.option(\"header\", \"true\").schema(manual_schema) \\\n",
    "    .csv(\"file:/Workspace/Shared/course_enrollments.csv\")\n",
    "\n",
    "df_manual.show()\n",
    "df_manual.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66b0dba-4c57-4703-b9dc-69d9d323cea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------------+-----------+----------+---------------+------+--------+\n|EnrollmentID|StudentName|         CourseName|   Category|EnrollDate|ProgressPercent|Rating|  Status|\n+------------+-----------+-------------------+-----------+----------+---------------+------+--------+\n|      ENR003|     Aakash|Power BI Essentials|  Analytics|2024-05-13|             30|   3.8|  Active|\n|      ENR004|       Neha|        Java Basics|Programming|2024-05-15|              0|  NULL|Inactive|\n+------------+-----------+-------------------+-----------+----------+---------------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#3. Filter records where ProgressPercent < 50\n",
    "df_filtered = df_infer.filter(df_infer.ProgressPercent < 50)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eceb7433-2fc3-4e75-8976-7951221bb370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+\n|EnrollmentID|StudentName|          CourseName|   Category|EnrollDate|ProgressPercent|           Rating|   Status|\n+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+\n|      ENR001|     Aditya|Python for Beginners|Programming|2024-05-10|             80|              4.5|   Active|\n|      ENR002|     Simran|Data Analysis wit...|  Analytics|2024-05-12|            100|              4.7|Completed|\n|      ENR003|     Aakash| Power BI Essentials|  Analytics|2024-05-13|             30|              3.8|   Active|\n|      ENR004|       Neha|         Java Basics|Programming|2024-05-15|              0|4.359999999999999| Inactive|\n|      ENR005|       Zara|Machine Learning 101|         AI|2024-05-17|             60|              4.2|   Active|\n|      ENR006|    Ibrahim|Python for Beginners|Programming|2024-05-18|             90|              4.6|Completed|\n+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#4. Replace null ratings with average rating\n",
    "from pyspark.sql.functions import avg, when, col\n",
    "\n",
    "avg_rating = df_infer.select(avg(\"Rating\")).first()[0]\n",
    "\n",
    "df_filled = df_infer.withColumn(\"Rating\", when(col(\"Rating\").isNull(), avg_rating).otherwise(col(\"Rating\")))\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec4dabb4-fd52-4539-b615-e2dc568b96a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+--------+\n|EnrollmentID|StudentName|          CourseName|   Category|EnrollDate|ProgressPercent|           Rating|   Status|IsActive|\n+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+--------+\n|      ENR001|     Aditya|Python for Beginners|Programming|2024-05-10|             80|              4.5|   Active|       1|\n|      ENR002|     Simran|Data Analysis wit...|  Analytics|2024-05-12|            100|              4.7|Completed|       0|\n|      ENR003|     Aakash| Power BI Essentials|  Analytics|2024-05-13|             30|              3.8|   Active|       1|\n|      ENR004|       Neha|         Java Basics|Programming|2024-05-15|              0|4.359999999999999| Inactive|       0|\n|      ENR005|       Zara|Machine Learning 101|         AI|2024-05-17|             60|              4.2|   Active|       1|\n|      ENR006|    Ibrahim|Python for Beginners|Programming|2024-05-18|             90|              4.6|Completed|       0|\n+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#5. Add column IsActive â†’ 1 if Status is Active, else 0\n",
    "df_active = df_filled.withColumn(\"IsActive\", when(col(\"Status\") == \"Active\", 1).otherwise(0))\n",
    "df_active.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beab13cd-6492-4a91-8f78-bd4046045d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|          CourseName|avg(ProgressPercent)|\n+--------------------+--------------------+\n|Data Analysis wit...|               100.0|\n|         Java Basics|                 0.0|\n|Machine Learning 101|                60.0|\n|Python for Beginners|                85.0|\n| Power BI Essentials|                30.0|\n+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#6. Find average progress by course\n",
    "df_active.groupBy(\"CourseName\").avg(\"ProgressPercent\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e358c9-5d7a-453c-a450-fa1e94287661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|   Category|count|\n+-----------+-----+\n|Programming|    3|\n|         AI|    1|\n|  Analytics|    2|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#7. Get count of students in each course category\n",
    "df_active.groupBy(\"Category\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98ff952-83e1-44e1-b1b7-066bd2fbe477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|          CourseName|count|\n+--------------------+-----+\n|Python for Beginners|    2|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#8. Identify the most enrolled course\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df_active.groupBy(\"CourseName\").count().orderBy(desc(\"count\")).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3382d3c-d74c-4713-8106-3d944ebd6236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+\n|          CourseName|DurationWeeks|Instructor|\n+--------------------+-------------+----------+\n|Python for Beginners|            4|    Rakesh|\n|Data Analysis wit...|            3|    Anjali|\n| Power BI Essentials|            5|     Rekha|\n|         Java Basics|            6|     Manoj|\n|Machine Learning 101|            8|     Samir|\n+--------------------+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#9. Load course_details.csv\n",
    "df_course_details = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file:/Workspace/Shared/course_details.csv\")\n",
    "\n",
    "df_course_details.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0810086c-eaf8-4e2a-b37d-7bc1c6837f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+\n|          CourseName|EnrollmentID|StudentName|   Category|EnrollDate|ProgressPercent|           Rating|   Status|IsActive|DurationWeeks|Instructor|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+\n|Python for Beginners|      ENR001|     Aditya|Programming|2024-05-10|             80|              4.5|   Active|       1|            4|    Rakesh|\n|Data Analysis wit...|      ENR002|     Simran|  Analytics|2024-05-12|            100|              4.7|Completed|       0|            3|    Anjali|\n| Power BI Essentials|      ENR003|     Aakash|  Analytics|2024-05-13|             30|              3.8|   Active|       1|            5|     Rekha|\n|         Java Basics|      ENR004|       Neha|Programming|2024-05-15|              0|4.359999999999999| Inactive|       0|            6|     Manoj|\n|Machine Learning 101|      ENR005|       Zara|         AI|2024-05-17|             60|              4.2|   Active|       1|            8|     Samir|\n|Python for Beginners|      ENR006|    Ibrahim|Programming|2024-05-18|             90|              4.6|Completed|       0|            4|    Rakesh|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#10. Join course_enrollments with course_details\n",
    "df_joined = df_active.join(df_course_details, on=\"CourseName\", how=\"left\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2400bf5-c3b4-4e9d-9b2e-f5576128360d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------------+----+\n|StudentName|          CourseName|ProgressPercent|Rank|\n+-----------+--------------------+---------------+----+\n|     Simran|Data Analysis wit...|            100|   1|\n|       Neha|         Java Basics|              0|   1|\n|       Zara|Machine Learning 101|             60|   1|\n|     Aakash| Power BI Essentials|             30|   1|\n|    Ibrahim|Python for Beginners|             90|   1|\n|     Aditya|Python for Beginners|             80|   2|\n+-----------+--------------------+---------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "#11. Rank students in each course based on ProgressPercent\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "windowSpec = Window.partitionBy(\"CourseName\").orderBy(desc(\"ProgressPercent\"))\n",
    "\n",
    "df_ranked = df_joined.withColumn(\"Rank\", rank().over(windowSpec))\n",
    "df_ranked.select(\"StudentName\", \"CourseName\", \"ProgressPercent\", \"Rank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86cb1a12-4eea-4be3-b7c5-b7e0723874c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------+--------------+--------------+\n|EnrollmentID|   Category|EnrollDate|NextEnrollDate|PrevEnrollDate|\n+------------+-----------+----------+--------------+--------------+\n|      ENR005|         AI|2024-05-17|          NULL|          NULL|\n|      ENR002|  Analytics|2024-05-12|    2024-05-13|          NULL|\n|      ENR003|  Analytics|2024-05-13|          NULL|    2024-05-12|\n|      ENR001|Programming|2024-05-10|    2024-05-15|          NULL|\n|      ENR004|Programming|2024-05-15|    2024-05-18|    2024-05-10|\n|      ENR006|Programming|2024-05-18|          NULL|    2024-05-15|\n+------------+-----------+----------+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#12. Get lead and lag of EnrollDate by Category\n",
    "from pyspark.sql.functions import lead, lag\n",
    "\n",
    "windowCat = Window.partitionBy(\"Category\").orderBy(\"EnrollDate\")\n",
    "\n",
    "df_lead_lag = df_joined.withColumn(\"NextEnrollDate\", lead(\"EnrollDate\").over(windowCat)) \\\n",
    "    .withColumn(\"PrevEnrollDate\", lag(\"EnrollDate\").over(windowCat))\n",
    "\n",
    "df_lead_lag.select(\"EnrollmentID\", \"Category\", \"EnrollDate\", \"NextEnrollDate\", \"PrevEnrollDate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a72f516-c89e-44b1-979f-d09f74909849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---------+--------+\n|   Category|Active|Completed|Inactive|\n+-----------+------+---------+--------+\n|Programming|     1|        1|       1|\n|         AI|     1|     NULL|    NULL|\n|  Analytics|     1|        1|    NULL|\n+-----------+------+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#13. Pivot data to show total enrollments by Category and Status\n",
    "df_pivot = df_joined.groupBy(\"Category\").pivot(\"Status\").count()\n",
    "df_pivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4432b35b-c2d7-4c62-affa-7fd893051a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+-----------+\n|EnrollmentID|EnrollDate|EnrollYear|EnrollMonth|\n+------------+----------+----------+-----------+\n|      ENR001|2024-05-10|      2024|          5|\n|      ENR002|2024-05-12|      2024|          5|\n|      ENR003|2024-05-13|      2024|          5|\n|      ENR004|2024-05-15|      2024|          5|\n|      ENR005|2024-05-17|      2024|          5|\n|      ENR006|2024-05-18|      2024|          5|\n+------------+----------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#14. Extract year and month from EnrollDate\n",
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "df_with_date = df_joined.withColumn(\"EnrollYear\", year(\"EnrollDate\")) \\\n",
    "                        .withColumn(\"EnrollMonth\", month(\"EnrollDate\"))\n",
    "\n",
    "df_with_date.select(\"EnrollmentID\", \"EnrollDate\", \"EnrollYear\", \"EnrollMonth\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e6be690-4d61-417a-b1be-92642d62a512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n|          CourseName|EnrollmentID|StudentName|   Category|EnrollDate|ProgressPercent|           Rating|   Status|IsActive|DurationWeeks|Instructor|EnrollYear|EnrollMonth|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n|Python for Beginners|      ENR001|     Aditya|Programming|2024-05-10|             80|              4.5|   Active|       1|            4|    Rakesh|      2024|          5|\n|Data Analysis wit...|      ENR002|     Simran|  Analytics|2024-05-12|            100|              4.7|Completed|       0|            3|    Anjali|      2024|          5|\n| Power BI Essentials|      ENR003|     Aakash|  Analytics|2024-05-13|             30|              3.8|   Active|       1|            5|     Rekha|      2024|          5|\n|         Java Basics|      ENR004|       Neha|Programming|2024-05-15|              0|4.359999999999999| Inactive|       0|            6|     Manoj|      2024|          5|\n|Machine Learning 101|      ENR005|       Zara|         AI|2024-05-17|             60|              4.2|   Active|       1|            8|     Samir|      2024|          5|\n|Python for Beginners|      ENR006|    Ibrahim|Programming|2024-05-18|             90|              4.6|Completed|       0|            4|    Rakesh|      2024|          5|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#15. Drop rows where Status is null or empty\n",
    "df_cleaned = df_with_date.filter((col(\"Status\").isNotNull()) & (col(\"Status\") != \"\"))\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7319b8dc-56c2-4249-8939-b685e5033fd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n|          CourseName|EnrollmentID|StudentName|   Category|EnrollDate|ProgressPercent|           Rating|   Status|IsActive|DurationWeeks|Instructor|EnrollYear|EnrollMonth|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n|         Java Basics|      ENR004|       Neha|Programming|2024-05-15|              0|4.359999999999999| Inactive|       0|            6|     Manoj|      2024|          5|\n|Machine Learning 101|      ENR005|       Zara|         AI|2024-05-17|             60|              4.2|   Active|       1|            8|     Samir|      2024|          5|\n|Data Analysis wit...|      ENR002|     Simran|  Analytics|2024-05-12|            100|              4.7|Completed|       0|            3|    Anjali|      2024|          5|\n| Power BI Essentials|      ENR003|     Aakash|  Analytics|2024-05-13|             30|              3.8|   Active|       1|            5|     Rekha|      2024|          5|\n|Python for Beginners|      ENR006|    Ibrahim|Programming|2024-05-18|             90|              4.6|Completed|       0|            4|    Rakesh|      2024|          5|\n|Python for Beginners|      ENR001|     Aditya|Programming|2024-05-10|             80|              4.5|   Active|       1|            4|    Rakesh|      2024|          5|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#16. Remove duplicate enrollments using dropDuplicates()\n",
    "df_no_duplicates = df_cleaned.dropDuplicates([\"EnrollmentID\"])\n",
    "df_no_duplicates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749a92ae-75f0-4de3-a71b-e28882a88d97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#17. Write final cleaned DataFrame to:\n",
    "# CSV \n",
    "df_no_duplicates.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"file:/Workspace/Shared/final_course_output_csv\")\n",
    "\n",
    "# JSON \n",
    "df_no_duplicates.write.mode(\"overwrite\") \\\n",
    "    .json(\"file:/Workspace/Shared/final_course_output_json\")\n",
    "\n",
    "# Parquet \n",
    "df_no_duplicates.write.mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(\"file:/Workspace/Shared/final_course_output_parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Azure PySpark Exercises-7",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}