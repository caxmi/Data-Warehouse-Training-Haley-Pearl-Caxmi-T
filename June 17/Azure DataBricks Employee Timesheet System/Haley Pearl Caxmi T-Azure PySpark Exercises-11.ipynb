{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c948cc-5c34-41e3-a1f7-a6679c7beaac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-4855800111011335887\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7369884f0850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "704b7793-9486-4821-96ad-ddbda0fea4fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|\n+----------+-----+----------+-------+---------+----------+---------+------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|\n+----------+-----+----------+-------+---------+----------+---------+------+\n\nroot\n |-- EmployeeID: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Project: string (nullable = true)\n |-- WorkHours: integer (nullable = true)\n |-- WorkDate: date (nullable = true)\n |-- Location: string (nullable = true)\n |-- Mode: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#1. Load the CSV using inferred schema\n",
    "\n",
    "df_auto = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"file:/Workspace/Shared/employee_timesheet.csv\")\n",
    "df_auto.show()\n",
    "df_auto.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a06325a-1919-443c-be80-538dda4fa5ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|\n+----------+-----+----------+-------+---------+----------+---------+------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|\n+----------+-----+----------+-------+---------+----------+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#2. Load the same file with schema explicitly defined\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", StringType()),\n",
    "    StructField(\"Name\", StringType()),\n",
    "    StructField(\"Department\", StringType()),\n",
    "    StructField(\"Project\", StringType()),\n",
    "    StructField(\"WorkHours\", IntegerType()),\n",
    "    StructField(\"WorkDate\", DateType()),\n",
    "    StructField(\"Location\", StringType()),\n",
    "    StructField(\"Mode\", StringType())\n",
    "])\n",
    "\n",
    "df_manual = spark.read.option(\"header\", \"true\").schema(schema).csv(\"file:/Workspace/Shared/employee_timesheet.csv\")\n",
    "df_manual.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2efbe5-93ce-4e01-81a3-a1a43a70f18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#3. Add a new column Weekday extracted from WorkDate\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "df = df_manual.withColumn(\"Weekday\", date_format(\"WorkDate\", \"EEEE\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ebb6e61-ed5e-4fd3-b0bb-45efdb1f1a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n|EmployeeID| Name|TotalHours|\n+----------+-----+----------+\n|      E103| John|         5|\n|      E104|Meena|         6|\n|      E102|  Raj|        15|\n|      E101|Anita|        17|\n+----------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#4. Calculate total work hours by employee\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.groupBy(\"EmployeeID\", \"Name\").agg(sum(\"WorkHours\").alias(\"TotalHours\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5d3e1e-e959-4c6e-88c8-e2a30643103a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n|Department|         AvgHours|\n+----------+-----------------+\n|        HR|              7.5|\n|   Finance|              5.0|\n|        IT|7.666666666666667|\n+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#5. Calculate average work hours per department\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.groupBy(\"Department\").agg(avg(\"WorkHours\").alias(\"AvgHours\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b961d22-aa9d-48dd-9dcc-457e29fd56d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+----+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|Rank|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----+\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|   1|\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|   2|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|   2|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----+\n\n"
     ]
    }
   ],
   "source": [
    "#6. Get top 2 employees by total hours using window function\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "windowSpec = Window.orderBy(df[\"WorkHours\"].desc())\n",
    "df.withColumn(\"Rank\", rank().over(windowSpec)).filter(\"Rank <= 2\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfeea3c-c18c-4888-aa47-796022c32250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+-------+---------+----------+--------+------+--------+\n|EmployeeID|Name|Department|Project|WorkHours|  WorkDate|Location|  Mode| Weekday|\n+----------+----+----------+-------+---------+----------+--------+------+--------+\n|      E102| Raj|        HR|   Beta|        8|2024-05-04|  Mumbai|Remote|Saturday|\n+----------+----+----------+-------+---------+----------+--------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#7. Filter entries where WorkDate falls on a weekend\n",
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "df.filter(dayofweek(\"WorkDate\").isin([1, 7])).show()  # Sunday=1, Saturday=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b090cbe7-4a39-41cf-aa63-ee269819308d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|RunningTotal|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|           8|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|          17|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|           7|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|          15|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|           5|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|           6|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#8. Running total of hours per employee using window\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec = Window.partitionBy(\"EmployeeID\").orderBy(\"WorkDate\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "df.withColumn(\"RunningTotal\", sum(\"WorkHours\").over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0e95294-e014-40ad-a2ec-011629c58c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n|Department|DeptHead|\n+----------+--------+\n|        IT|   Anand|\n|        HR|  Shruti|\n|   Finance|   Kamal|\n+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#9. Create department_location.csv (manually using DataFrame)\n",
    "from pyspark.sql import Row\n",
    "\n",
    "dept_data = [Row(Department=\"IT\", DeptHead=\"Anand\"),\n",
    "             Row(Department=\"HR\", DeptHead=\"Shruti\"),\n",
    "             Row(Department=\"Finance\", DeptHead=\"Kamal\")]\n",
    "\n",
    "df_dept = spark.createDataFrame(dept_data)\n",
    "df_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62abc00-bfbc-4c9b-8da8-d1f9291fc103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+--------+\n|EmployeeID| Name|Department|DeptHead|\n+----------+-----+----------+--------+\n|      E101|Anita|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n|      E103| John|   Finance|   Kamal|\n|      E101|Anita|        IT|   Anand|\n|      E104|Meena|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n+----------+-----+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#10. Join with timesheet data and list all employees with their DeptHead\n",
    "\n",
    "df.join(df_dept, on=\"Department\", how=\"left\").select(\"EmployeeID\", \"Name\", \"Department\", \"DeptHead\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7237e20-2757-41a8-acdb-a4c2c4148b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+\n|EmployeeID|Alpha|Beta|Gamma|\n+----------+-----+----+-----+\n|      E103|    5|NULL| NULL|\n|      E104| NULL|NULL|    6|\n|      E101|   17|NULL| NULL|\n|      E102| NULL|  15| NULL|\n+----------+-----+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#11. Pivot table: total hours per employee per project\n",
    "\n",
    "df.groupBy(\"EmployeeID\").pivot(\"Project\").sum(\"WorkHours\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ab40c8-62c8-4088-834a-0403e2d5fe0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n|EmployeeID|  Mode|Hours|\n+----------+------+-----+\n|      E103|Remote|    5|\n|      E103|Onsite|    0|\n|      E104|Remote|    0|\n|      E104|Onsite|    6|\n|      E101|Remote|   17|\n|      E101|Onsite|    0|\n|      E102|Remote|    8|\n|      E102|Onsite|    7|\n+----------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#12. Unpivot example: Convert mode-specific hours into rows\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Assume these are the pivoted columns; create manually\n",
    "pivoted = df.groupBy(\"EmployeeID\").pivot(\"Mode\").sum(\"WorkHours\").fillna(0)\n",
    "unpivoted = pivoted.selectExpr(\"EmployeeID\", \"stack(2, 'Remote', Remote, 'Onsite', Onsite) as (Mode, Hours)\")\n",
    "unpivoted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26dfca52-17bf-45a6-a3c3-1fb3613c404d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#13. Create UDF to classify work hours\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def workload_tag(hours):\n",
    "    if hours >= 8:\n",
    "        return \"Full\"\n",
    "    elif hours >= 4:\n",
    "        return \"Partial\"\n",
    "    else:\n",
    "        return \"Light\"\n",
    "\n",
    "workload_udf = udf(workload_tag, StringType())\n",
    "df.withColumn(\"WorkloadCategory\", workload_udf(\"WorkHours\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169f4b02-3f77-4256-8e17-c8f9bfe84bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#14. Add WorkloadCategory using this UDF\n",
    "df.withColumn(\"WorkloadCategory\", workload_udf(\"WorkHours\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "778c7ad2-b511-4e2a-b49e-c9c08ce24c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|  NULL|Wednesday|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|  NULL| Saturday|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#15. Introduce nulls in Mode column (for demonstration)\n",
    "\n",
    "df_null = df.withColumn(\"Mode\", expr(\"CASE WHEN EmployeeID = 'E102' THEN NULL ELSE Mode END\"))\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a5b6a6-dd2b-481a-bd45-2e01122171a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------------+---------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|  Weekday|\n+----------+-----+----------+-------+---------+----------+---------+------------+---------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      Remote|Wednesday|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Not Provided|Wednesday|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote| Thursday|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|      Remote|   Friday|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|      Onsite|   Friday|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Not Provided| Saturday|\n+----------+-----+----------+-------+---------+----------+---------+------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#16. Fill nulls with \"Not Provided\"\n",
    "df_filled = df_null.fillna({\"Mode\": \"Not Provided\"})\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560cbdf2-537a-43ec-ab2a-ca3c0982a454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#17. Drop rows where WorkHours < 4\n",
    "\n",
    "df_filtered = df.filter(df.WorkHours >= 4)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6faaba4-1f2b-4781-b1a4-aad3b484766b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+\n|EmployeeID|RemoteRatio|RemoteWorker|\n+----------+-----------+------------+\n|      E103|        1.0|         Yes|\n|      E104|        0.0|          No|\n|      E101|        1.0|         Yes|\n|      E102|        0.5|          No|\n+----------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#18. Mark \"Remote Worker\" if >80% entries are Remote\n",
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "df.groupBy(\"EmployeeID\").agg(\n",
    "    (sum(when(df.Mode == \"Remote\", 1).otherwise(0)) / count(\"*\")).alias(\"RemoteRatio\")\n",
    ").withColumn(\"RemoteWorker\", when(expr(\"RemoteRatio >= 0.8\"), \"Yes\").otherwise(\"No\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74860434-1e20-4e48-b517-e3589708e128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+----------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|ExtraHours|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|         0|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|         0|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         0|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|         1|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|         0|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|         0|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#19. Add new column ExtraHours where hours > 8\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df.withColumn(\"ExtraHours\", when(df.WorkHours > 8, df.WorkHours - 8).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cc15a17-b2b7-469c-82a0-fe6518111411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+-------+---------+----------+---------+------+---------+\n|EmployeeID|   Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|\n+----------+-------+----------+-------+---------+----------+---------+------+---------+\n|      E101|  Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|\n|      E102|    Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|\n|      E103|   John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|\n|      E101|  Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|\n|      E104|  Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|\n|      E102|    Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|\n|      E999|Intern1|        IT|  Alpha|        5|2024-05-05|  Chennai|Onsite|     NULL|\n+----------+-------+----------+-------+---------+----------+---------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#20. Append dummy timesheet using unionByName()\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "new_data = [(\"E999\", \"Intern1\", \"IT\", \"Alpha\", 5, \"2024-05-05\", \"Chennai\", \"Onsite\")]\n",
    "columns = [\"EmployeeID\", \"Name\", \"Department\", \"Project\", \"WorkHours\", \"WorkDate\", \"Location\", \"Mode\"]\n",
    "\n",
    "df_dummy = spark.createDataFrame(new_data, columns)\n",
    "df_dummy = df_dummy.withColumn(\"WorkDate\", df_dummy[\"WorkDate\"].cast(\"date\"))\n",
    "\n",
    "df_combined = df.unionByName(df_dummy, allowMissingColumns=True)\n",
    "df_combined.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c962a861-bbec-4540-8b67-62c5e13dd717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+-------+---------+----------+---------+------+---------+\n|EmployeeID|   Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|\n+----------+-------+----------+-------+---------+----------+---------+------+---------+\n|      E101|  Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|\n|      E102|    Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|\n|      E102|    Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|\n|      E101|  Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|\n|      E103|   John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|\n|      E104|  Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|\n|      E999|Intern1|        IT|  Alpha|        5|2024-05-05|  Chennai|Onsite|     NULL|\n+----------+-------+----------+-------+---------+----------+---------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#21. Remove duplicate rows\n",
    "\n",
    "df_combined.dropDuplicates().show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Azure PySpark Exercises-11",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}