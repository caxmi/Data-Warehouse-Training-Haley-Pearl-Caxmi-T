{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8efc32-1d18-4c29-98cd-b9b62e5accd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-4855800111011335887\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7bca7a6e47d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f37ee3b6-999e-4e4f-9888-5973f6694e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- TransactionID: string (nullable = true)\n |-- Customer: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Product: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- UnitPrice: integer (nullable = true)\n |-- TotalPrice: integer (nullable = true)\n |-- TransactionDate: date (nullable = true)\n |-- PaymentMode: string (nullable = true)\n\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|    70000|     70000|     2024-01-15|       Card|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       2|    30000|     60000|     2024-01-20|        UPI|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|    15000|     15000|     2024-02-10|Net Banking|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     5000|     20000|     2024-02-12|       Card|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|    50000|     50000|     2024-02-15|       Card|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|     1000|      3000|     2024-02-18|       Cash|\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#1. Load retail_data.csv into a PySpark DataFrame and display schema\n",
    "\n",
    "df_retail = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file:/Workspace/Shared/retail_data.csv\")\n",
    "\n",
    "df_retail.printSchema()\n",
    "df_retail.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cce867-4a5a-4d0b-972f-a8026421d348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- TransactionID: string (nullable = true)\n |-- Customer: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Product: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- UnitPrice: integer (nullable = true)\n |-- TotalPrice: integer (nullable = true)\n |-- TransactionDate: string (nullable = true)\n |-- PaymentMode: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#2. Infer schema as False â€” then manually cast columns\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "manual_schema = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),\n",
    "    StructField(\"Customer\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Product\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", IntegerType(), True),\n",
    "    StructField(\"TotalPrice\", IntegerType(), True),\n",
    "    StructField(\"TransactionDate\", StringType(), True),\n",
    "    StructField(\"PaymentMode\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_manual = spark.read.option(\"header\", \"true\").schema(manual_schema) \\\n",
    "    .csv(\"file:/Workspace/Shared/retail_data.csv\")\n",
    "\n",
    "df_manual.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d51ef1-6e08-404c-a3db-35820f7125ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|    70000|     70000|     2024-01-15|       Card|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       2|    30000|     60000|     2024-01-20|        UPI|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|    50000|     50000|     2024-02-15|       Card|\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#3. Filter transactions where TotalPrice > 40000\n",
    "df_retail.filter(df_retail.TotalPrice > 40000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f002b27-2ea3-4e52-81db-37d74a4e5c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|     City|\n+---------+\n|Bangalore|\n|   Mumbai|\n|    Delhi|\n|Hyderabad|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#4. Get unique cities from the dataset\n",
    "\n",
    "df_retail.select(\"City\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291d2d37-b430-4088-a3e4-0a845cf83015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n|TransactionID|Customer| City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n|        T1004|    Zoya|Delhi|  Chair|  Furniture|       4|     5000|     20000|     2024-02-12|       Card|\n|        T1006|   Farah|Delhi|  Mouse|Electronics|       3|     1000|      3000|     2024-02-18|       Cash|\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n|TransactionID|Customer| City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n|        T1004|    Zoya|Delhi|  Chair|  Furniture|       4|     5000|     20000|     2024-02-12|       Card|\n|        T1006|   Farah|Delhi|  Mouse|Electronics|       3|     1000|      3000|     2024-02-18|       Cash|\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#5. Find all transactions from \"Delhi\" using .filter() and .where()\n",
    "# Using filter()\n",
    "df_retail.filter(df_retail.City == \"Delhi\").show()\n",
    "\n",
    "# Using where()\n",
    "df_retail.where(df_retail.City == \"Delhi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0ecb94-2757-4454-a40a-db9b1b84ad9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------------+\n|TransactionID|TotalPrice|DiscountedPrice|\n+-------------+----------+---------------+\n|        T1001|     70000|        63000.0|\n|        T1002|     60000|        54000.0|\n|        T1003|     15000|        13500.0|\n|        T1004|     20000|        18000.0|\n|        T1005|     50000|        45000.0|\n|        T1006|      3000|         2700.0|\n+-------------+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#6. Add a column DiscountedPrice = TotalPrice - 10%\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_retail = df_retail.withColumn(\"DiscountedPrice\", col(\"TotalPrice\") * 0.9)\n",
    "df_retail.select(\"TransactionID\", \"TotalPrice\", \"DiscountedPrice\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7277fe77-6e4c-49e6-a629-afae538b7115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- TransactionID: string (nullable = true)\n |-- Customer: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Product: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- UnitPrice: integer (nullable = true)\n |-- TotalPrice: integer (nullable = true)\n |-- TxnDate: date (nullable = true)\n |-- PaymentMode: string (nullable = true)\n |-- DiscountedPrice: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#7. Rename TransactionDate to TxnDate\n",
    "\n",
    "df_retail = df_retail.withColumnRenamed(\"TransactionDate\", \"TxnDate\")\n",
    "df_retail.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48e5f14-fd0b-453c-8dd2-27353956b8b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- TransactionID: string (nullable = true)\n |-- Customer: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Product: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- TotalPrice: integer (nullable = true)\n |-- TxnDate: date (nullable = true)\n |-- PaymentMode: string (nullable = true)\n |-- DiscountedPrice: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#8. Drop the column UnitPrice\n",
    "df_retail = df_retail.drop(\"UnitPrice\")\n",
    "df_retail.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9758dd7-8c0b-4909-b197-cb94858204a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n|     City|TotalSales|\n+---------+----------+\n|Bangalore|     60000|\n|   Mumbai|    120000|\n|    Delhi|     23000|\n|Hyderabad|     15000|\n+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#9. Get total sales by city\n",
    "df_retail.groupBy(\"City\").sum(\"TotalPrice\").withColumnRenamed(\"sum(TotalPrice)\", \"TotalSales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22e202e-c509-44ff-b9ce-e230e9db587e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n|   Category|AvgUnitPrice|\n+-----------+------------+\n|Electronics|     37750.0|\n|  Furniture|     10000.0|\n+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#10. Get average unit price by category (need original with UnitPrice)\n",
    "\n",
    "df_full = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file:/Workspace/Shared/retail_data.csv\")\n",
    "\n",
    "df_full.groupBy(\"Category\").avg(\"UnitPrice\").withColumnRenamed(\"avg(UnitPrice)\", \"AvgUnitPrice\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319226a2-fe71-419a-8099-eb2e9402b41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|PaymentMode|count|\n+-----------+-----+\n|Net Banking|    1|\n|       Card|    3|\n|       Cash|    1|\n|        UPI|    1|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#11. Count of transactions grouped by PaymentMode\n",
    "\n",
    "df_retail.groupBy(\"PaymentMode\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c28bbe-6e90-4b0c-8cda-6c6a427ade22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+----+\n|TransactionID|     City|TotalPrice|Rank|\n+-------------+---------+----------+----+\n|        T1002|Bangalore|     60000|   1|\n|        T1004|    Delhi|     20000|   1|\n|        T1006|    Delhi|      3000|   2|\n|        T1003|Hyderabad|     15000|   1|\n|        T1001|   Mumbai|     70000|   1|\n|        T1005|   Mumbai|     50000|   2|\n+-------------+---------+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "#12. Rank transactions by TotalPrice within each City\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "win_spec = Window.partitionBy(\"City\").orderBy(col(\"TotalPrice\").desc())\n",
    "\n",
    "df_retail.withColumn(\"Rank\", rank().over(win_spec)).select(\"TransactionID\", \"City\", \"TotalPrice\", \"Rank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b6b84c7-4e0f-463a-8f70-ebdd4752cc86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+----------+\n|TransactionID|     City|TotalPrice|PrevAmount|\n+-------------+---------+----------+----------+\n|        T1002|Bangalore|     60000|      NULL|\n|        T1004|    Delhi|     20000|      NULL|\n|        T1006|    Delhi|      3000|     20000|\n|        T1003|Hyderabad|     15000|      NULL|\n|        T1001|   Mumbai|     70000|      NULL|\n|        T1005|   Mumbai|     50000|     70000|\n+-------------+---------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#13. Use lag() to get previous transaction amount per city\n",
    "\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "win_spec_lag = Window.partitionBy(\"City\").orderBy(\"TxnDate\")\n",
    "\n",
    "df_retail.withColumn(\"PrevAmount\", lag(\"TotalPrice\").over(win_spec_lag)) \\\n",
    "    .select(\"TransactionID\", \"City\", \"TotalPrice\", \"PrevAmount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b71f7726-5f59-4e45-a294-6820ea55aa42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n|     City|Region|\n+---------+------+\n|   Mumbai|  West|\n|    Delhi| North|\n|Bangalore| South|\n|Hyderabad| South|\n+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#14. Create a second DataFrame: city_region\n",
    "from pyspark.sql import Row\n",
    "\n",
    "city_region = spark.createDataFrame([\n",
    "    Row(City=\"Mumbai\", Region=\"West\"),\n",
    "    Row(City=\"Delhi\", Region=\"North\"),\n",
    "    Row(City=\"Bangalore\", Region=\"South\"),\n",
    "    Row(City=\"Hyderabad\", Region=\"South\")\n",
    "])\n",
    "city_region.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db15aad2-0f7e-42e2-a1a9-8fc75c5e6c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|Region|TotalSales|\n+------+----------+\n| South|     75000|\n|  West|    120000|\n| North|     23000|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#15. Join and get total sales by Region\n",
    "\n",
    "df_joined = df_retail.join(city_region, on=\"City\", how=\"left\")\n",
    "df_joined.groupBy(\"Region\").sum(\"TotalPrice\").withColumnRenamed(\"sum(TotalPrice)\", \"TotalSales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41012dd-2c45-4220-a63d-1f7874e7ca73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|     70000|2024-01-15|       Card|        63000.0|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       1|     60000|2024-01-20|        UPI|        54000.0|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|     15000|2024-02-10|Net Banking|        13500.0|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     20000|2024-02-12|       Card|        18000.0|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|     50000|2024-02-15|       Card|        45000.0|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|      3000|2024-02-18|       Cash|         2700.0|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#16. Introduce nulls and replace them\n",
    "# Introduce nulls (simulate for demo)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_nulls = df_retail.withColumn(\"Quantity\", when(df_retail.TransactionID == \"T1002\", None).otherwise(df_retail.Quantity))\n",
    "\n",
    "# Replace with default values\n",
    "df_cleaned = df_nulls.fillna({\"Quantity\": 1})\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c5ac16d-4186-4a9b-8980-11703b8060f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|     70000|2024-01-15|       Card|        63000.0|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|     15000|2024-02-10|Net Banking|        13500.0|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     20000|2024-02-12|       Card|        18000.0|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|     50000|2024-02-15|       Card|        45000.0|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|      3000|2024-02-18|       Cash|         2700.0|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#17. Drop rows where Quantity is null\n",
    "df_cleaned = df_nulls.na.drop(subset=[\"Quantity\"])\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f3cadf3-1a60-420d-b262-517c34828186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n|TransactionID|PaymentMode|\n+-------------+-----------+\n|        T1001|       Card|\n|        T1003|Net Banking|\n|        T1004|       Card|\n|        T1005|       Card|\n|        T1006|       Cash|\n+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#18. Fill null PaymentMode with \"Unknown\"\n",
    "\n",
    "df_filled = df_cleaned.fillna({\"PaymentMode\": \"Unknown\"})\n",
    "df_filled.select(\"TransactionID\", \"PaymentMode\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc4bb9a-6a2e-4799-bc0c-f405e19a7249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------+\n|TransactionID|TotalPrice|OrderLabel|\n+-------------+----------+----------+\n|        T1001|     70000|      High|\n|        T1003|     15000|       Low|\n|        T1004|     20000|       Low|\n|        T1005|     50000|    Medium|\n|        T1006|      3000|       Low|\n+-------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#19. Label orders using UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def label_order(amount):\n",
    "    if amount > 50000:\n",
    "        return \"High\"\n",
    "    elif amount >= 30000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "label_udf = udf(label_order, StringType())\n",
    "\n",
    "df_labeled = df_filled.withColumn(\"OrderLabel\", label_udf(col(\"TotalPrice\")))\n",
    "df_labeled.select(\"TransactionID\", \"TotalPrice\", \"OrderLabel\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375caa76-df7e-4005-be24-394ceaca869e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----+-----+---+\n|TransactionID|   TxnDate|Year|Month|Day|\n+-------------+----------+----+-----+---+\n|        T1001|2024-01-15|2024|    1| 15|\n|        T1003|2024-02-10|2024|    2| 10|\n|        T1004|2024-02-12|2024|    2| 12|\n|        T1005|2024-02-15|2024|    2| 15|\n|        T1006|2024-02-18|2024|    2| 18|\n+-------------+----------+----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "#20. Extract year, month, and day from TxnDate\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth, to_date\n",
    "\n",
    "df_dates = df_labeled.withColumn(\"TxnDate\", to_date(\"TxnDate\"))\n",
    "df_dates = df_dates.withColumn(\"Year\", year(\"TxnDate\")) \\\n",
    "                   .withColumn(\"Month\", month(\"TxnDate\")) \\\n",
    "                   .withColumn(\"Day\", dayofmonth(\"TxnDate\"))\n",
    "df_dates.select(\"TransactionID\", \"TxnDate\", \"Year\", \"Month\", \"Day\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75700153-8a68-409e-86be-1cc4d46ae3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|OrderLabel|Year|Month|Day|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|     15000|2024-02-10|Net Banking|        13500.0|       Low|2024|    2| 10|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     20000|2024-02-12|       Card|        18000.0|       Low|2024|    2| 12|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|     50000|2024-02-15|       Card|        45000.0|    Medium|2024|    2| 15|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|      3000|2024-02-18|       Cash|         2700.0|       Low|2024|    2| 18|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "#Q21. Filter transactions in February\n",
    "df_dates.filter(month(\"TxnDate\") == 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f7602a-dd76-4c0a-8c52-30284c83b10a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|OrderLabel|Year|Month|Day|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|      3000|2024-02-18|       Cash|         2700.0|       Low|2024|    2| 18|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     20000|2024-02-12|       Card|        18000.0|       Low|2024|    2| 12|\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|     70000|2024-01-15|       Card|        63000.0|      High|2024|    1| 15|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|     15000|2024-02-10|Net Banking|        13500.0|       Low|2024|    2| 10|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|     50000|2024-02-15|       Card|        45000.0|    Medium|2024|    2| 15|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "#22. Duplicate using union() and remove duplicates\n",
    "df_union = df_dates.union(df_dates)\n",
    "df_distinct = df_union.dropDuplicates()\n",
    "df_distinct.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Azure PySpark Exercises-10",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}