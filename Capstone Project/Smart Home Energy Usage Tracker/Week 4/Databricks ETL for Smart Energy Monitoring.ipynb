{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1516fc48-f9c6-42cf-9268-87e5d0aafc72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-4215250180632747023\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x73bd7c981790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b91197e-5c71-42ed-8f77-922d0ac84438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------+-------+-------+-------+\n|device_id|          timestamp|energy_kwh|voltage|current|room_id|\n+---------+-------------------+----------+-------+-------+-------+\n|        4|2025-05-30 10:12:00|      3.24|  218.5|    4.8|      1|\n|        7|2025-05-30 11:15:00|      0.75|  221.3|    2.1|      2|\n|        2|2025-05-30 12:20:00|      1.12|  219.7|    3.3|      3|\n|        5|2025-05-30 13:25:00|      2.54|  217.9|    5.1|      4|\n|        1|2025-05-30 14:30:00|       4.1|  222.0|    7.0|      5|\n+---------+-------------------+----------+-------+-------+-------+\nonly showing top 5 rows\n\nroot\n |-- device_id: integer (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- energy_kwh: double (nullable = true)\n |-- voltage: double (nullable = true)\n |-- current: double (nullable = true)\n |-- room_id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#1.Upload cleaned logs to Databricks\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"file:/Workspace/Shared/cleaned_energy_logs.csv\", inferSchema=True)\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e35afcd-22d0-4501-8ce9-481a2370e4bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------------+\n|      date|room_id|daily_energy_kwh|\n+----------+-------+----------------+\n|2025-05-30|      2|             2.5|\n|2025-05-30|      1|            7.19|\n|2025-05-30|      4|            4.74|\n|2025-05-30|      3|            1.67|\n|2025-05-30|      5|            7.43|\n+----------+-------+----------------+\n\n+----+-------+-----------------+\n|week|room_id|weekly_energy_kwh|\n+----+-------+-----------------+\n|  22|      5|             7.43|\n|  22|      2|              2.5|\n|  22|      4|             4.74|\n|  22|      1|             7.19|\n|  22|      3|             1.67|\n+----+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#2.Build ETL pipeline to calculate daily/weekly summaries\n",
    "\n",
    "from pyspark.sql.functions import to_date, to_timestamp, weekofyear, sum, round\n",
    "\n",
    "# Timestamp conversion and new columns\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "df = df.withColumn(\"date\", to_date(\"timestamp\"))\n",
    "df = df.withColumn(\"week\", weekofyear(\"timestamp\"))\n",
    "\n",
    "# Daily Summary per Room\n",
    "daily_summary = df.groupBy(\"date\", \"room_id\") \\\n",
    "    .agg(round(sum(\"energy_kwh\"), 2).alias(\"daily_energy_kwh\"))\n",
    "\n",
    "# Weekly Summary per Room\n",
    "weekly_summary = df.groupBy(\"week\", \"room_id\") \\\n",
    "    .agg(round(sum(\"energy_kwh\"), 2).alias(\"weekly_energy_kwh\"))\n",
    "\n",
    "daily_summary.show()\n",
    "weekly_summary.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a392dd3c-51ec-4ba2-824f-11757a7febbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3.Save Final Results in Delta & CSV Format\n",
    "# Save as Delta\n",
    "daily_summary.write.format(\"delta\").mode(\"overwrite\").save(\"file:/Workspace/Shared/delta/daily_summary\")\n",
    "weekly_summary.write.format(\"delta\").mode(\"overwrite\").save(\"file:/Workspace/Shared/delta/weekly_summary\")\n",
    "\n",
    "# Save as CSV\n",
    "daily_summary.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"file:/Workspace/Shared/output/daily_summary_csv\")\n",
    "weekly_summary.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"file:/Workspace/Shared/output/weekly_summary_csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27a733f6-3e14-472a-a134-607f90675374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+\n|device_id|date|daily_kwh|\n+---------+----+---------+\n+---------+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#4.Optional â€“ SQL query for devices exceeding 10 kWh/day\n",
    "# Register view\n",
    "df.createOrReplaceTempView(\"energy_logs\")\n",
    "\n",
    "# SQL: devices using >10 kWh in a day\n",
    "spark.sql(\"\"\"\n",
    "SELECT device_id, date, ROUND(SUM(energy_kwh), 2) AS daily_kwh\n",
    "FROM energy_logs\n",
    "GROUP BY device_id, date\n",
    "HAVING daily_kwh > 10\n",
    "\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks ETL for Smart Energy Monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}