# -*- coding: utf-8 -*-
"""Spark for device level aggregation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BpeNopbfmWZ_tgw--2KDQpBuMhG5rrnp
"""

#1. Load a large dataset of sensor logs using PySpark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DeviceAggregation").getOrCreate()
spark

# Load cleaned sensor logs CSV
df = spark.read.option("header", "true").csv("cleaned_energy_logs.csv", inferSchema=True)

# Display sample data
df.show(5)
df.printSchema()

#2.Group by device and calculate peak vs off-peak usage
from pyspark.sql.functions import hour, when, to_timestamp, sum

df = df.withColumn("timestamp", to_timestamp("timestamp"))
df = df.withColumn("hour", hour("timestamp"))

# Classify as Peak (6 PM to 6 AM) or Off-Peak (6 AM to 6 PM)
df = df.withColumn("usage_period", when((df["hour"] >= 18) | (df["hour"] < 6), "Peak").otherwise("Off-Peak"))

# Group by device and period, aggregate total energy
peak_offpeak_usage = df.groupBy("device_id", "usage_period").agg(
    sum("energy_kwh").alias("total_energy_kwh")
)

peak_offpeak_usage.show()

#3.Identify top energy-consuming devices

top_devices = df.groupBy("device_id").agg(
    sum("energy_kwh").alias("total_usage_kwh")
).orderBy("total_usage_kwh", ascending=False)

top_devices.show(10)

#Save the output

peak_offpeak_usage.write.option("header", "true").mode("overwrite").csv("output/peak_offpeak_usage")
top_devices.write.option("header", "true").mode("overwrite").csv("output/top_energy_devices")