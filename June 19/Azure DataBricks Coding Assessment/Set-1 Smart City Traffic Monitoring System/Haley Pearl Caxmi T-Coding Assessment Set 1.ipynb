{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e55cd7c-5225-4e37-9bd8-a128d94c7d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-3854842569157478638\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fea677e1610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1274e2-f57c-4c74-bcb9-855ec62c1c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- LogID: string (nullable = true)\n |-- VehicleID: string (nullable = true)\n |-- EntryPoint: string (nullable = true)\n |-- ExitPoint: string (nullable = true)\n |-- EntryTime: timestamp (nullable = true)\n |-- ExitTime: timestamp (nullable = true)\n |-- VehicleType: string (nullable = true)\n |-- SpeedKMH: integer (nullable = true)\n |-- TollPaid: integer (nullable = true)\n\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|\n| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|\n| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|\n| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n\nroot\n |-- LogID: string (nullable = true)\n |-- VehicleID: string (nullable = true)\n |-- EntryPoint: string (nullable = true)\n |-- ExitPoint: string (nullable = true)\n |-- EntryTime: timestamp (nullable = true)\n |-- ExitTime: timestamp (nullable = true)\n |-- VehicleType: string (nullable = true)\n |-- SpeedKMH: integer (nullable = true)\n |-- TollPaid: integer (nullable = true)\n\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|\n| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|\n| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|\n| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#1 Data Ingestion & Schema Analysis\n",
    "# Load CSV using PySpark with schema inference\n",
    "file_path = \"file:/Workspace/Shared/traffic_logs.csv\"\n",
    "\n",
    "df_auto = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm\")\n",
    "    .csv(file_path)\n",
    ")\n",
    "\n",
    "# Show inferred schema and data\n",
    "df_auto.printSchema()\n",
    "df_auto.show()\n",
    "\n",
    "# Manually define schema and compare\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "manual_schema = StructType([\n",
    "    StructField(\"LogID\", StringType(), True),\n",
    "    StructField(\"VehicleID\", StringType(), True),\n",
    "    StructField(\"EntryPoint\", StringType(), True),\n",
    "    StructField(\"ExitPoint\", StringType(), True),\n",
    "    StructField(\"EntryTime\", TimestampType(), True),\n",
    "    StructField(\"ExitTime\", TimestampType(), True),\n",
    "    StructField(\"VehicleType\", StringType(), True),\n",
    "    StructField(\"SpeedKMH\", IntegerType(), True),\n",
    "    StructField(\"TollPaid\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "df_manual = spark.read.option(\"header\", \"true\").schema(manual_schema).csv(file_path)\n",
    "df_manual.printSchema()\n",
    "df_manual.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807ecd1d-c4eb-48f7-8112-b97041ae83ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|                 19|      false|\n| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|                 35|      false|\n| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|                 18|      false|\n| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|                 20|       true|\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|                 35|      false|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#2 Derived Column Creation\n",
    "# Add TripDurationMinutes and IsOverspeed columns\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "df_derived = df_manual.withColumn(\n",
    "    \"TripDurationMinutes\",\n",
    "    (expr(\"unix_timestamp(ExitTime) - unix_timestamp(EntryTime)\") / 60).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"IsOverspeed\",\n",
    "    col(\"SpeedKMH\") > 60\n",
    ")\n",
    "\n",
    "df_derived.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f5cdc7-2c61-4fda-8373-8c5c5579a49f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n|VehicleType|AvgSpeed|\n+-----------+--------+\n|       Bike|    55.0|\n|        Car|    70.0|\n|      Truck|    45.0|\n|        Bus|    40.0|\n+-----------+--------+\n\n+----------+---------+\n|EntryPoint|TotalToll|\n+----------+---------+\n|     GateA|       80|\n|     GateB|      170|\n|     GateC|       50|\n+----------+---------+\n\n+---------+---------+\n|ExitPoint|TripCount|\n+---------+---------+\n|    GateD|        2|\n+---------+---------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "#3 Vehicle Behavior Aggregations\n",
    "from pyspark.sql.functions import avg, sum, count, col\n",
    "\n",
    "# Average speed per VehicleType\n",
    "df_derived.groupBy(\"VehicleType\").agg(avg(\"SpeedKMH\").alias(\"AvgSpeed\")).show()\n",
    "\n",
    "# Total toll collected per EntryPoint\n",
    "df_derived.groupBy(\"EntryPoint\").agg(sum(\"TollPaid\").alias(\"TotalToll\")).show()\n",
    "\n",
    "# Most used ExitPoint\n",
    "df_derived.groupBy(\"ExitPoint\").agg(count(\"*\").alias(\"TripCount\")).orderBy(col(\"TripCount\").desc()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26074102-4d68-4643-bb94-f16a1b076c3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|SpeedRank|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+\n| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|                 18|      false|        1|\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|                 35|      false|        1|\n| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|                 20|       true|        1|\n| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|                 19|      false|        2|\n| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|                 35|      false|        1|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+\n\n+---------+-------------------+-------------------+------------+\n|VehicleID|          EntryTime|           ExitTime|LastExitTime|\n+---------+-------------------+-------------------+------------+\n|     V001|2024-05-01 08:01:00|2024-05-01 08:20:00|        NULL|\n|     V002|2024-05-01 08:10:00|2024-05-01 08:45:00|        NULL|\n|     V003|2024-05-01 09:00:00|2024-05-01 09:18:00|        NULL|\n|     V004|2024-05-01 09:15:00|2024-05-01 09:35:00|        NULL|\n|     V005|2024-05-01 10:05:00|2024-05-01 10:40:00|        NULL|\n+---------+-------------------+-------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#4 Window Functions\n",
    "\n",
    "# Rank vehicles by speed within VehicleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "w = Window.partitionBy(\"VehicleType\").orderBy(col(\"SpeedKMH\").desc())\n",
    "df_ranked = df_derived.withColumn(\"SpeedRank\", rank().over(w))\n",
    "df_ranked.show()\n",
    "\n",
    "# Find last exit time for each vehicle using lag()\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "w_vehicle = Window.partitionBy(\"VehicleID\").orderBy(\"EntryTime\")\n",
    "\n",
    "df_lagged = df_derived.withColumn(\"LastExitTime\", lag(\"ExitTime\").over(w_vehicle))\n",
    "df_lagged.select(\"VehicleID\", \"EntryTime\", \"ExitTime\", \"LastExitTime\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33ddee0-4d08-43dd-9116-735ed8618579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+------------+---------------+\n|VehicleID|          EntryTime|           ExitTime|LastExitTime|IdleTimeMinutes|\n+---------+-------------------+-------------------+------------+---------------+\n|     V001|2024-05-01 08:01:00|2024-05-01 08:20:00|        NULL|           NULL|\n|     V002|2024-05-01 08:10:00|2024-05-01 08:45:00|        NULL|           NULL|\n|     V003|2024-05-01 09:00:00|2024-05-01 09:18:00|        NULL|           NULL|\n|     V004|2024-05-01 09:15:00|2024-05-01 09:35:00|        NULL|           NULL|\n|     V005|2024-05-01 10:05:00|2024-05-01 10:40:00|        NULL|           NULL|\n+---------+-------------------+-------------------+------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#5 Session Segmentation\n",
    "# Calculate idle time between trips\n",
    "from pyspark.sql.functions import unix_timestamp, round\n",
    "\n",
    "df_sessions = df_lagged.withColumn(\n",
    "    \"IdleTimeMinutes\",\n",
    "    round((unix_timestamp(\"EntryTime\") - unix_timestamp(\"LastExitTime\")) / 60, 2)\n",
    ")\n",
    "\n",
    "df_sessions.select(\"VehicleID\", \"EntryTime\", \"ExitTime\", \"LastExitTime\", \"IdleTimeMinutes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c6fd338-f38c-4ff9-b4bc-a02406554692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n|LogID|VehicleID|EntryPoint|ExitPoint|EntryTime|ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n|LogID|VehicleID|EntryPoint|ExitPoint|EntryTime|ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|                 35|      false|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#6 Anomaly Detection\n",
    "# Speed > 70 and TripDuration < 10\n",
    "df_anomaly1 = df_derived.filter((col(\"SpeedKMH\") > 70) & (col(\"TripDurationMinutes\") < 10))\n",
    "df_anomaly1.show()\n",
    "\n",
    "# Paid less toll for longer trips\n",
    "df_anomaly2 = df_derived.filter((col(\"TripDurationMinutes\") > 30) & (col(\"TollPaid\") < 50))\n",
    "df_anomaly2.show()\n",
    "\n",
    "# Suspicious backtracking (ExitPoint < EntryPoint)\n",
    "df_anomaly3 = df_derived.filter(col(\"ExitPoint\") < col(\"EntryPoint\"))\n",
    "df_anomaly3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a6aeba-d907-4f23-b388-eabdfc8fa6b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n|RegisteredCity|count|\n+--------------+-----+\n|     Bangalore|    1|\n|       Chennai|    1|\n|        Mumbai|    1|\n|          Pune|    1|\n|         Delhi|    1|\n+--------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#7 Join with Metadata\n",
    "# Create vehicle registry and join\n",
    "from pyspark.sql import Row\n",
    "\n",
    "vehicle_data = [\n",
    "    Row(VehicleID=\"V001\", OwnerName=\"Anil\", Model=\"Hyundai i20\", RegisteredCity=\"Delhi\"),\n",
    "    Row(VehicleID=\"V002\", OwnerName=\"Rakesh\", Model=\"Tata Truck\", RegisteredCity=\"Chennai\"),\n",
    "    Row(VehicleID=\"V003\", OwnerName=\"Sana\", Model=\"Yamaha R15\", RegisteredCity=\"Mumbai\"),\n",
    "    Row(VehicleID=\"V004\", OwnerName=\"Neha\", Model=\"Honda City\", RegisteredCity=\"Bangalore\"),\n",
    "    Row(VehicleID=\"V005\", OwnerName=\"Zoya\", Model=\"Volvo Bus\", RegisteredCity=\"Pune\"),\n",
    "]\n",
    "\n",
    "df_registry = spark.createDataFrame(vehicle_data)\n",
    "\n",
    "# Join and group trips by RegisteredCity\n",
    "df_joined = df_derived.join(df_registry, on=\"VehicleID\", how=\"left\")\n",
    "df_joined.groupBy(\"RegisteredCity\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f86be4c-0a54-4905-ad1a-21f698af56ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|     11|2025-06-19 05:53:39|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3072619134359810}|0611-043435-vg20yowf|          9|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     10|2025-06-19 05:53:37|8778822765517627|azuser3551_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{3072619134359810}|0611-043435-vg20yowf|          9|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      9|2025-06-19 05:53:36|8778822765517627|azuser3551_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3072619134359810}|0611-043435-vg20yowf|          8|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      8|2025-06-19 05:53:33|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3072619134359810}|0611-043435-vg20yowf|          7|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      7|2025-06-19 04:39:06|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3072619134359810}|0611-043435-vg20yowf|          5|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      6|2025-06-19 04:39:04|8778822765517627|azuser3551_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{3072619134359810}|0611-043435-vg20yowf|          5|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      5|2025-06-19 04:39:03|8778822765517627|azuser3551_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3072619134359810}|0611-043435-vg20yowf|          4|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      4|2025-06-19 04:39:00|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3072619134359810}|0611-043435-vg20yowf|          3|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      3|2025-06-19 04:35:16|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3072619134359810}|0611-043435-vg20yowf|          1|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      2|2025-06-19 04:35:14|8778822765517627|azuser3551_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{3072619134359810}|0611-043435-vg20yowf|          1|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      1|2025-06-19 04:35:11|8778822765517627|azuser3551_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3072619134359810}|0611-043435-vg20yowf|          0|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      0|2025-06-19 04:35:01|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3072619134359810}|0611-043435-vg20yowf|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|                 19|      false|\n| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|                 35|      false|\n| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|                 18|      false|\n| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|                 20|       true|\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|                 35|      false|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#8 Delta Lake Features\n",
    "# Save as Delta Table\n",
    "df_derived.write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Shared/traffic_delta\")\n",
    "\n",
    "# MERGE INTO: Update toll for bikes\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "traffic_delta = DeltaTable.forPath(spark, \"/Workspace/Shared/traffic_delta\")\n",
    "\n",
    "traffic_delta.alias(\"old\").merge(\n",
    "    df_derived.filter(col(\"VehicleType\") == \"Bike\").withColumn(\"TollPaid\", expr(\"TollPaid + 10\")).alias(\"new\"),\n",
    "    \"old.LogID = new.LogID\"\n",
    ").whenMatchedUpdate(set={\"TollPaid\": \"new.TollPaid\"}).execute()\n",
    "\n",
    "# Delete long trips > 60 mins\n",
    "traffic_delta.delete(\"TripDurationMinutes > 60\")\n",
    "\n",
    "# Delta versioning and history\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/Workspace/Shared/traffic_delta`\").show()\n",
    "\n",
    "# Read version 0 of delta table\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/Workspace/Shared/traffic_delta\")\n",
    "df_v0.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bafe60e1-c420-43b9-b2bc-73d3c8f516d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|TripType|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|                 19|      false|  Medium|\n| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|                 35|      false|    Long|\n| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|                 18|      false|  Medium|\n| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|                 20|       true|  Medium|\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|                 35|      false|    Long|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n\n+---------+--------+-----+\n|VehicleID|TripDate|count|\n+---------+--------+-----+\n+---------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#9 Advanced Conditions\n",
    "\n",
    "# Classify trip type\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_trip_types = df_derived.withColumn(\n",
    "    \"TripType\",\n",
    "    when(col(\"TripDurationMinutes\") < 15, \"Short\")\n",
    "    .when(col(\"TripDurationMinutes\").between(15, 30), \"Medium\")\n",
    "    .otherwise(\"Long\")\n",
    ")\n",
    "\n",
    "df_trip_types.show()\n",
    "\n",
    "# Flag vehicles with > 3 trips/day\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df_flagged = df_trip_types.withColumn(\"TripDate\", to_date(\"EntryTime\"))\n",
    "\n",
    "df_trip_count = df_flagged.groupBy(\"VehicleID\", \"TripDate\").count().filter(\"count > 3\")\n",
    "df_trip_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa18c541-4e54-4e54-98c8-203d506bb585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+\n|VehicleType|ExitPoint|TotalToll|\n+-----------+---------+---------+\n|        Car|    GateD|       50|\n|      Truck|    GateC|      100|\n|       Bike|    GateD|       30|\n|        Bus|    GateA|       70|\n|        Car|    GateC|       50|\n+-----------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#10 Export & Reporting\n",
    "\n",
    "# Save to Parquet partitioned by VehicleType\n",
    "df_derived.write.mode(\"overwrite\").partitionBy(\"VehicleType\").parquet(\"/Workspace/Shared/traffic_parquet\")\n",
    "\n",
    "# Save to CSV for dashboard\n",
    "df_derived.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Workspace/Shared/traffic_dashboard_csv\")\n",
    "\n",
    "# Summary SQL View: total toll by VehicleType & ExitPoint\n",
    "df_derived.createOrReplaceTempView(\"traffic_summary\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT VehicleType, ExitPoint, SUM(TollPaid) as TotalToll\n",
    "    FROM traffic_summary\n",
    "    GROUP BY VehicleType, ExitPoint\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Coding Assessment Set 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}