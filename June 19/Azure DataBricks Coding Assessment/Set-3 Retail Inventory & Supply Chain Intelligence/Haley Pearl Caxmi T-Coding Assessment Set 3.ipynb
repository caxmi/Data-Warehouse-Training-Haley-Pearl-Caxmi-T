{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fa0691-2fcf-4173-b8ab-f4e8e251ec7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-3854842569157478638\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1bb0d248d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a75c44-fd87-441c-950a-883c5cca8e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|   AVTech|\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000|  ChairCo|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+\n\n+---------+-----+\n|Warehouse|count|\n+---------+-----+\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 1: Inventory Alerting System\n",
    "\n",
    "# 1. Load the data using PySpark.\n",
    "df_inventory = (\n",
    "    spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
    "    .csv(\"file:/Workspace/Shared/inventory_supply.csv\")\n",
    ")\n",
    "df_inventory.show()\n",
    "# 2. Create a new column NeedsReorder = StockQty < ReorderLevel\n",
    "from pyspark.sql.functions import col\n",
    "df_inventory = df_inventory.withColumn(\"NeedsReorder\", col(\"StockQty\") < col(\"ReorderLevel\"))\n",
    "\n",
    "# 3. Create a view of all items that need restocking\n",
    "df_inventory.filter(\"NeedsReorder = true\").createOrReplaceTempView(\"items_to_restock\")\n",
    "\n",
    "# 4. Highlight warehouses with more than 2 such items\n",
    "df_inventory.filter(\"NeedsReorder = true\") \\\n",
    "    .groupBy(\"Warehouse\").count().filter(\"count > 2\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb80d303-2125-4272-9ae1-89072f81882c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n| Supplier|AvgPrice|\n+---------+--------+\n|   AVTech| 30000.0|\n|TechWorld| 70000.0|\n|PrintFast|  8000.0|\n| FreezeIt| 25000.0|\n|  ChairCo|  6000.0|\n+---------+--------+\n\n+---------+------------------+--------+\n| Supplier|     BelowAvgRatio|GoodDeal|\n+---------+------------------+--------+\n|   AVTech|1.0000000000000000|    true|\n|TechWorld|0.0000000000000000|   false|\n|PrintFast|1.0000000000000000|    true|\n| FreezeIt|0.0000000000000000|   false|\n|  ChairCo|0.0000000000000000|   false|\n+---------+------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 2: Supplier Price Optimization\n",
    "\n",
    "# 1. Group items by Supplier and compute average price\n",
    "df_supplier_avg = df_inventory.groupBy(\"Supplier\").avg(\"UnitPrice\").withColumnRenamed(\"avg(UnitPrice)\", \"AvgPrice\")\n",
    "df_supplier_avg.show()\n",
    "\n",
    "# 2. Find which suppliers offer items below average price in their category\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, expr\n",
    "\n",
    "cat_window = Window.partitionBy(\"Category\")\n",
    "df_inventory = df_inventory.withColumn(\"CategoryAvg\", avg(\"UnitPrice\").over(cat_window))\n",
    "df_inventory = df_inventory.withColumn(\"BelowCategoryAvg\", col(\"UnitPrice\") < col(\"CategoryAvg\"))\n",
    "\n",
    "# 3. Tag suppliers with Good Deal if >50% of their items are below market average\n",
    "df_good_deals = df_inventory.groupBy(\"Supplier\").agg(\n",
    "    (expr(\"sum(case when BelowCategoryAvg then 1 else 0 end) * 1.0 / count(*)\")).alias(\"BelowAvgRatio\")\n",
    ").withColumn(\"GoodDeal\", col(\"BelowAvgRatio\") > 0.5)\n",
    "\n",
    "df_good_deals.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e3f2f74-b74f-4936-9f7f-8bf8da871c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n|    ItemName|TotalStockValue|\n+------------+---------------+\n|      LED TV|        1500000|\n|      Laptop|         700000|\n|Office Chair|         240000|\n+------------+---------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 3: Cost Forecasting\n",
    "\n",
    "# 1. Calculate TotalStockValue = StockQty * UnitPrice\n",
    "df_inventory = df_inventory.withColumn(\"TotalStockValue\", col(\"StockQty\") * col(\"UnitPrice\"))\n",
    "\n",
    "# 2. Identify top 3 highest-value items\n",
    "df_inventory.orderBy(col(\"TotalStockValue\").desc()).select(\"ItemName\", \"TotalStockValue\").show(3)\n",
    "\n",
    "# 3. Export the result as a Parquet file partitioned by Warehouse\n",
    "df_inventory.write.mode(\"overwrite\").partitionBy(\"Warehouse\").parquet(\"file:/Workspace/Shared/inventory_cost_forecast\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "640feb7c-a0ed-4de8-bbdb-7fba63781bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n| Warehouse|ItemCount|\n+----------+---------+\n|WarehouseA|        2|\n|WarehouseC|        1|\n|WarehouseB|        2|\n+----------+---------+\n\n+----------+-----------+--------+\n| Warehouse|   Category|AvgStock|\n+----------+-----------+--------+\n|WarehouseB|Electronics|     6.5|\n|WarehouseA|  Furniture|    40.0|\n|WarehouseC| Appliances|     5.0|\n|WarehouseA|Electronics|    50.0|\n+----------+-----------+--------+\n\n+----------+----------+\n| Warehouse|TotalStock|\n+----------+----------+\n|WarehouseA|        90|\n|WarehouseC|         5|\n|WarehouseB|        13|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 4: Warehouse Utilization\n",
    "\n",
    "# 1. Count items stored per warehouse\n",
    "df_inventory.groupBy(\"Warehouse\").count().withColumnRenamed(\"count\", \"ItemCount\").show()\n",
    "\n",
    "# 2. Average stock per category in each warehouse\n",
    "df_inventory.groupBy(\"Warehouse\", \"Category\").avg(\"StockQty\").withColumnRenamed(\"avg(StockQty)\", \"AvgStock\").show()\n",
    "\n",
    "# 3. Determine underutilized warehouses (total stock < 100)\n",
    "df_inventory.groupBy(\"Warehouse\").sum(\"StockQty\").withColumnRenamed(\"sum(StockQty)\", \"TotalStock\") \\\n",
    "    .filter(\"TotalStock < 100\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50565651-98d3-4871-8d51-354303a12c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      7|2025-06-19 05:16:...|8778822765517627|azuser3551_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{3072619134359928}|0611-043435-vg20yowf|          6|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      6|2025-06-19 05:16:...|8778822765517627|azuser3551_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{3072619134359928}|0611-043435-vg20yowf|          5|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      5|2025-06-19 05:16:...|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3072619134359928}|0611-043435-vg20yowf|          4|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      4|2025-06-19 05:16:...|8778822765517627|azuser3551_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3072619134359928}|0611-043435-vg20yowf|          3|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      3|2025-06-19 05:16:...|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3072619134359928}|0611-043435-vg20yowf|          1|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      2|2025-06-19 05:16:...|8778822765517627|azuser3551_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{3072619134359928}|0611-043435-vg20yowf|          1|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      1|2025-06-19 05:16:...|8778822765517627|azuser3551_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{3072619134359928}|0611-043435-vg20yowf|          0|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      0|2025-06-19 05:16:...|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3072619134359928}|0611-043435-vg20yowf|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+--------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-----------+----------------+---------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|CategoryAvg|BelowCategoryAvg|TotalStockValue|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-----------+----------------+---------------+\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|        true|    25000.0|           false|         125000|\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|   AVTech|       false|    36000.0|            true|        1500000|\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|        true|    36000.0|           false|         700000|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|        true|    36000.0|            true|          24000|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000|  ChairCo|       false|     6000.0|           false|         240000|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-----------+----------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 5: Delta Audit Trail\n",
    "\n",
    "# 1. Save as Delta table retail_inventory\n",
    "df_inventory.write.format(\"delta\").mode(\"overwrite\").save(\"file:/Workspace/Shared/retail_inventory\")\n",
    "\n",
    "# 2. Update stock of 'Laptop' to 20\n",
    "from delta.tables import DeltaTable\n",
    "delta_tbl = DeltaTable.forPath(spark, \"file:/Workspace/Shared/retail_inventory\")\n",
    "delta_tbl.update(condition=\"ItemName = 'Laptop'\", set={\"StockQty\": \"20\"})\n",
    "\n",
    "# 3. Delete any item with StockQty = 0\n",
    "delta_tbl.delete(condition=\"StockQty = 0\")\n",
    "\n",
    "# 4. Run DESCRIBE HISTORY and query VERSION AS OF previous state\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`file:/Workspace/Shared/retail_inventory`\").show()\n",
    "prev_ver = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"file:/Workspace/Shared/retail_inventory\")\n",
    "prev_ver.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2a5992-5abe-4883-8f6e-b8b4d533da8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------+-------------+-----------+-----------------+\n|ItemID|    ItemName|StockQty|QuantityAdded|NewStockQty|RestockedRecently|\n+------+------------+--------+-------------+-----------+-----------------+\n|  I001|      LED TV|      50|           20|         70|             true|\n|  I002|      Laptop|      10|           10|         20|             true|\n|  I005|     Printer|       3|            5|          8|             true|\n|  I003|Office Chair|      40|         NULL|         40|            false|\n|  I004|Refrigerator|       5|         NULL|          5|            false|\n+------+------------+--------+-------------+-----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 6: Alerts from Restock Logs\n",
    "\n",
    "# 1. Load restock_logs.csv and join to update StockQty\n",
    "df_logs = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file:/Workspace/Shared/restock_logs.csv\")\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_updated = df_inventory.join(df_logs, \"ItemID\", \"left\") \\\n",
    "    .withColumn(\"NewStockQty\", expr(\"StockQty + coalesce(QuantityAdded, 0)\")) \\\n",
    "    .withColumn(\"RestockedRecently\", col(\"QuantityAdded\").isNotNull())\n",
    "\n",
    "df_updated.select(\"ItemID\", \"ItemName\", \"StockQty\", \"QuantityAdded\", \"NewStockQty\", \"RestockedRecently\").show()\n",
    "\n",
    "# 2. Use MERGE INTO to update in Delta\n",
    "delta_tbl.alias(\"target\").merge(\n",
    "    df_updated.alias(\"source\"),\n",
    "    \"target.ItemID = source.ItemID\"\n",
    ").whenMatchedUpdate(set={\"StockQty\": \"source.NewStockQty\"}).execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2a246cd-fb41-4d80-a320-138883f689d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-----------+----------------+---------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|CategoryAvg|BelowCategoryAvg|TotalStockValue|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-----------+----------------+---------------+\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|        true|    25000.0|           false|         125000|\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|   AVTech|       false|    36000.0|            true|        1500000|\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|        true|    36000.0|           false|         700000|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|        true|    36000.0|            true|          24000|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000|  ChairCo|       false|     6000.0|           false|         240000|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-----------+----------------+---------------+\n\n+---------+------------+\n| Supplier|AvgUnitPrice|\n+---------+------------+\n|TechWorld|     70000.0|\n|   AVTech|     30000.0|\n| FreezeIt|     25000.0|\n|PrintFast|      8000.0|\n|  ChairCo|      6000.0|\n+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 7: Report Generation with SQL Views\n",
    "\n",
    "# 1. Create SQL view inventory_summary\n",
    "df_inventory = df_inventory.withColumn(\"TotalStockValue\", col(\"StockQty\") * col(\"UnitPrice\"))\n",
    "df_inventory.createOrReplaceTempView(\"inventory_summary\")\n",
    "spark.sql(\"SELECT * FROM inventory_summary\").show()\n",
    "\n",
    "# 2. Create supplier_leaderboard sorted by average price\n",
    "df_inventory.groupBy(\"Supplier\").avg(\"UnitPrice\") \\\n",
    "    .withColumnRenamed(\"avg(UnitPrice)\", \"AvgUnitPrice\") \\\n",
    "    .orderBy(\"AvgUnitPrice\", ascending=False) \\\n",
    "    .createOrReplaceTempView(\"supplier_leaderboard\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM supplier_leaderboard\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e2e129-276e-40a2-9634-1a6b1b63da62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-----------+----------------+---------------+-------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|CategoryAvg|BelowCategoryAvg|TotalStockValue|StockCategory|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-----------+----------------+---------------+-------------+\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|        true|    25000.0|           false|         125000|     LowStock|\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|        true|    36000.0|           false|         700000|     LowStock|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|        true|    36000.0|            true|          24000|     LowStock|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-----------+----------------+---------------+-------------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+--------+------------+-----------+----------------+---------------+-------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice|Supplier|NeedsReorder|CategoryAvg|BelowCategoryAvg|TotalStockValue|StockCategory|\n+------+------------+-----------+----------+--------+------------+-------------+---------+--------+------------+-----------+----------------+---------------+-------------+\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|  AVTech|       false|    36000.0|            true|        1500000|  Overstocked|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000| ChairCo|       false|     6000.0|           false|         240000|  Overstocked|\n+------+------------+-----------+----------+--------+------------+-------------+---------+--------+------------+-----------+----------------+---------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 8: Advanced Filtering\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# 1. Categorize items using when/otherwise\n",
    "df_inventory = df_inventory.withColumn(\"StockCategory\", when(col(\"StockQty\") > 2 * col(\"ReorderLevel\"), \"Overstocked\")\n",
    "                                       .otherwise(\"LowStock\"))\n",
    "\n",
    "# 2. Use .filter() and .where()\n",
    "df_inventory.filter(\"StockCategory = 'LowStock'\").show()\n",
    "df_inventory.where(\"StockCategory = 'Overstocked'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5180a25c-d5aa-4acb-9236-9c453f2ecc34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+---------+\n|    ItemName|StockAge|AgeBucket|\n+------------+--------+---------+\n|      LED TV|     461|    Stale|\n|      Laptop|     444|    Stale|\n|Office Chair|     451|    Stale|\n|Refrigerator|     485|    Stale|\n|     Printer|     446|    Stale|\n+------------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 9: Feature Engineering\n",
    "\n",
    "from pyspark.sql.functions import to_date, month, current_date, datediff\n",
    "\n",
    "# 1. Extract RestockMonth from LastRestocked\n",
    "df_inventory = df_inventory.withColumn(\"RestockMonth\", month(to_date(\"LastRestocked\")))\n",
    "\n",
    "# 2. Create StockAge = CURRENT_DATE - LastRestocked\n",
    "df_inventory = df_inventory.withColumn(\"StockAge\", datediff(current_date(), to_date(\"LastRestocked\")))\n",
    "\n",
    "# 3. Bucket StockAge\n",
    "df_inventory = df_inventory.withColumn(\"AgeBucket\",\n",
    "    when(col(\"StockAge\") <= 30, \"New\")\n",
    "    .when(col(\"StockAge\") <= 90, \"Moderate\")\n",
    "    .otherwise(\"Stale\")\n",
    ")\n",
    "\n",
    "df_inventory.select(\"ItemName\", \"StockAge\", \"AgeBucket\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41049b02-eabb-45cc-b719-a4a07f10b7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scenario 10: Export Options\n",
    "\n",
    "# 1. Write full DataFrame to CSV for analysts\n",
    "df_inventory.write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
    "    .csv(\"file:/Workspace/Shared/export/inventory/csv_output\")\n",
    "\n",
    "# 2. Write to JSON for integration\n",
    "df_inventory.write.mode(\"overwrite\").json(\"file:/Workspace/Shared/export/inventory/json_output\")\n",
    "\n",
    "# 3. Save as Delta for pipelines\n",
    "df_inventory.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .save(\"file:/Workspace/Shared/export/inventory/delta_output\")\n",
    "\n",
    "# Save stale items separately\n",
    "df_inventory.filter(\"AgeBucket = 'Stale'\") \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .parquet(\"file:/Workspace/Shared/export/inventory/stale_items\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Coding Assessment Set 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}