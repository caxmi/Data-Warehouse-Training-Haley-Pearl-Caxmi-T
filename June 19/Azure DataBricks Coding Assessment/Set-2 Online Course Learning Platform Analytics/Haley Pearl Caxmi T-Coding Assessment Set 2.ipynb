{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "302a21b9-8e01-4e20-9fdb-b1ffb06f1801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-3854842569157478638\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7aeb3c33c890>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1270ab-714c-4e44-bdfb-809d63745023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|DaysToComplete|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|     4|             9|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|    2024-04-18|             45|     3|            16|\n|    E003|  U001|    C003|  ML with PySpark|Data Science|2024-04-03|    2024-05-02|             30|     3|            29|\n|    E004|  U003|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|     5|            16|\n|    E005|  U004|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|     4|            11|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#1 Ingestion & Time Fields\n",
    "\n",
    "# Load into PySpark with inferred schema\n",
    "df_enroll = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"file:/Workspace/Shared/course_enrollments.csv\")\n",
    ")\n",
    "\n",
    "# Convert EnrollDate and CompletionDate to date type\n",
    "from pyspark.sql.functions import to_date, datediff\n",
    "\n",
    "df_enroll = df_enroll.withColumn(\"EnrollDate\", to_date(\"EnrollDate\")) \\\n",
    "                     .withColumn(\"CompletionDate\", to_date(\"CompletionDate\"))\n",
    "\n",
    "# Add DaysToComplete column if completed\n",
    "df_enroll = df_enroll.withColumn(\"DaysToComplete\", datediff(\"CompletionDate\", \"EnrollDate\"))\n",
    "\n",
    "df_enroll.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d6471e-6601-4ede-bc5d-eae3f59dbbc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-----------+\n|UserID|CoursesEnrolled|AvgProgress|\n+------+---------------+-----------+\n|  U004|              1|      100.0|\n|  U002|              1|       45.0|\n|  U003|              1|      100.0|\n|  U001|              2|       65.0|\n+------+---------------+-----------+\n\n+--------+------+---------------+-----------+\n|EnrollID|UserID|ProgressPercent|IsCompleted|\n+--------+------+---------------+-----------+\n|    E001|  U001|            100|       true|\n|    E002|  U002|             45|      false|\n|    E003|  U001|             30|      false|\n|    E004|  U003|            100|       true|\n|    E005|  U004|            100|       true|\n+--------+------+---------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#2 User Learning Path Progress\n",
    "\n",
    "# Group by UserID: count of courses enrolled and avg progress\n",
    "from pyspark.sql.functions import col, avg, count\n",
    "\n",
    "df_user_progress = df_enroll.groupBy(\"UserID\").agg(\n",
    "    count(\"*\").alias(\"CoursesEnrolled\"),\n",
    "    avg(\"ProgressPercent\").alias(\"AvgProgress\")\n",
    ")\n",
    "\n",
    "df_user_progress.show()\n",
    "\n",
    "# Flag IsCompleted = ProgressPercent == 100\n",
    "df_enroll = df_enroll.withColumn(\"IsCompleted\", col(\"ProgressPercent\") == 100)\n",
    "df_enroll.select(\"EnrollID\", \"UserID\", \"ProgressPercent\", \"IsCompleted\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fc30f4-0359-4a26-ad44-d3e6ba3a0d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+------+---------------+\n|EnrollID|ProgressPercent|Rating|EngagementScore|\n+--------+---------------+------+---------------+\n|    E001|            100|     4|            400|\n|    E002|             45|     3|            135|\n|    E003|             30|     3|             90|\n|    E004|            100|     5|            500|\n|    E005|            100|     4|            400|\n+--------+---------------+------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#3 Engagement Scoring\n",
    "# Replace null Rating with 0 and create EngagementScore = ProgressPercent * Rating\n",
    "from pyspark.sql.functions import expr, coalesce\n",
    "\n",
    "df_enroll = df_enroll.withColumn(\"Rating\", coalesce(col(\"Rating\"), expr(\"0\"))) \\\n",
    "                     .withColumn(\"EngagementScore\", col(\"ProgressPercent\") * col(\"Rating\"))\n",
    "\n",
    "df_enroll.select(\"EnrollID\", \"ProgressPercent\", \"Rating\", \"EngagementScore\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c8f68a9-1660-4d52-a59c-c1f2bcbed382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+----------+--------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|EnrollID|UserID|CourseID|CourseName|Category|EnrollDate|CompletionDate|ProgressPercent|Rating|DaysToComplete|IsCompleted|EngagementScore|\n+--------+------+--------+----------+--------+----------+--------------+---------------+------+--------------+-----------+---------------+\n+--------+------+--------+----------+--------+----------+--------------+---------------+------+--------------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#4 Identify Drop-offs\n",
    "from pyspark.sql.functions import col, trim, when\n",
    "df_enroll = df_enroll.withColumn(\n",
    "    \"CompletionDate\",\n",
    "    when(trim(col(\"CompletionDate\")) == \"\", None).otherwise(col(\"CompletionDate\"))\n",
    ")\n",
    "# Filter records with ProgressPercent < 50 and CompletionDate is null\n",
    "df_dropouts = df_enroll.filter(\n",
    "    (col(\"ProgressPercent\") < 50) & (col(\"CompletionDate\").isNull())\n",
    ")\n",
    "# Create a view called Dropouts\n",
    "df_dropouts.createOrReplaceTempView(\"Dropouts\")\n",
    "spark.sql(\"SELECT * FROM Dropouts\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83713116-05cd-4ddc-9c4f-ba3de55f477f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n|   Instructor|AvgProgress|\n+-------------+-----------+\n|  Zoya Sheikh|      100.0|\n|   Sana Gupta|       45.0|\n| Ibrahim Khan|       30.0|\n|Abdullah Khan|      100.0|\n+-------------+-----------+\n\n+-------------+-------------+-----------+\n|   CourseName|   Instructor|Enrollments|\n+-------------+-------------+-----------+\n|Python Basics|Abdullah Khan|          2|\n+-------------+-------------+-----------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "#5 Joins with Metadata\n",
    "\n",
    "# Create course_catalog as DataFrame\n",
    "from pyspark.sql import Row\n",
    "\n",
    "course_catalog = [\n",
    "    Row(CourseID=\"C001\", Instructor=\"Abdullah Khan\", DurationHours=8, Level=\"Beginner\"),\n",
    "    Row(CourseID=\"C002\", Instructor=\"Sana Gupta\", DurationHours=5, Level=\"Beginner\"),\n",
    "    Row(CourseID=\"C003\", Instructor=\"Ibrahim Khan\", DurationHours=10, Level=\"Intermediate\"),\n",
    "    Row(CourseID=\"C004\", Instructor=\"Zoya Sheikh\", DurationHours=6, Level=\"Beginner\"),\n",
    "]\n",
    "\n",
    "df_catalog = spark.createDataFrame(course_catalog)\n",
    "\n",
    "# Join enrollments with catalog\n",
    "df_joined = df_enroll.join(df_catalog, on=\"CourseID\", how=\"left\")\n",
    "\n",
    "# Average progress per instructor\n",
    "df_joined.groupBy(\"Instructor\").agg(avg(\"ProgressPercent\").alias(\"AvgProgress\")).show()\n",
    "\n",
    "# Most enrolled course and instructor\n",
    "df_joined.groupBy(\"CourseName\", \"Instructor\").agg(count(\"*\").alias(\"Enrollments\")) \\\n",
    "    .orderBy(col(\"Enrollments\").desc()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a143993a-b1d9-4b3e-a7e2-f3e5e6a5f97b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|     26|2025-06-19 06:13:41|8778822765517627|azuser3551_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{3072619134359878}|0611-043435-vg20yowf|         25|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     25|2025-06-19 06:13:40|8778822765517627|azuser3551_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{3072619134359878}|0611-043435-vg20yowf|         24|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     24|2025-06-19 06:13:38|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3072619134359878}|0611-043435-vg20yowf|         23|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|     23|2025-06-19 06:07:19|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3072619134359878}|0611-043435-vg20yowf|         22|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     22|2025-06-19 06:07:17|8778822765517627|azuser3551_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{3072619134359878}|0611-043435-vg20yowf|         21|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     21|2025-06-19 06:07:16|8778822765517627|azuser3551_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{3072619134359878}|0611-043435-vg20yowf|         20|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     20|2025-06-19 06:07:14|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3072619134359878}|0611-043435-vg20yowf|         19|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|     19|2025-06-19 06:06:48|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3072619134359878}|0611-043435-vg20yowf|         18|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     18|2025-06-19 06:06:46|8778822765517627|azuser3551_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{3072619134359878}|0611-043435-vg20yowf|         17|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     17|2025-06-19 06:06:45|8778822765517627|azuser3551_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{3072619134359878}|0611-043435-vg20yowf|         16|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     16|2025-06-19 06:06:43|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3072619134359878}|0611-043435-vg20yowf|         15|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|     15|2025-06-19 06:06:34|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3072619134359878}|0611-043435-vg20yowf|         14|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     14|2025-06-19 06:06:32|8778822765517627|azuser3551_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{3072619134359878}|0611-043435-vg20yowf|         13|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     13|2025-06-19 06:06:31|8778822765517627|azuser3551_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{3072619134359878}|0611-043435-vg20yowf|         12|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     12|2025-06-19 06:06:29|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3072619134359878}|0611-043435-vg20yowf|         11|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|     11|2025-06-19 06:03:09|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3072619134359878}|0611-043435-vg20yowf|         10|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|     10|2025-06-19 06:03:07|8778822765517627|azuser3551_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{3072619134359878}|0611-043435-vg20yowf|          9|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      9|2025-06-19 06:03:06|8778822765517627|azuser3551_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{3072619134359878}|0611-043435-vg20yowf|          8|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      8|2025-06-19 06:03:04|8778822765517627|azuser3551_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3072619134359878}|0611-043435-vg20yowf|          7|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      7|2025-06-19 04:58:30|8778822765517627|azuser3551_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3072619134359878}|0611-043435-vg20yowf|          5|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#6 Delta Lake Practice\n",
    "\n",
    "# Save as Delta Table\n",
    "df_enroll.write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Shared/enrollments_delta\")\n",
    "\n",
    "# Update: Set all ratings to 5 where Course = 'Python Basics'\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/Workspace/Shared/enrollments_delta\")\n",
    "\n",
    "delta_table.update(\n",
    "    condition=col(\"CourseName\") == \"Python Basics\",\n",
    "    set={\"Rating\": expr(\"5\")}\n",
    ")\n",
    "\n",
    "# Delete rows where ProgressPercent = 0\n",
    "delta_table.delete(condition=col(\"ProgressPercent\") == 0)\n",
    "\n",
    "# Describe Delta history\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/Workspace/Shared/enrollments_delta`\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d34378e-4ac5-4695-bbee-3119a0f94fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----+\n|CourseID|EnrollCount|Rank|\n+--------+-----------+----+\n|    C001|          2|   1|\n|    C004|          1|   2|\n|    C003|          1|   2|\n|    C002|          1|   2|\n+--------+-----------+----+\n\n+------+-----------------+----------+---------------+\n|UserID|       CourseName|EnrollDate|     NextCourse|\n+------+-----------------+----------+---------------+\n|  U001|    Python Basics|2024-04-01|ML with PySpark|\n|  U001|  ML with PySpark|2024-04-03|           NULL|\n|  U002|Excel for Finance|2024-04-02|           NULL|\n|  U003|    Python Basics|2024-04-04|           NULL|\n|  U004|Digital Marketing|2024-04-05|           NULL|\n+------+-----------------+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#7 Window Functions\n",
    "\n",
    "# dense_rank(): Rank courses by number of enrollments\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "df_course_count = df_enroll.groupBy(\"CourseID\").agg(count(\"*\").alias(\"EnrollCount\"))\n",
    "\n",
    "w_rank = Window.orderBy(col(\"EnrollCount\").desc())\n",
    "df_course_ranked = df_course_count.withColumn(\"Rank\", dense_rank().over(w_rank))\n",
    "\n",
    "df_course_ranked.show()\n",
    "\n",
    "# lead(): Next course per user sorted by EnrollDate\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "w_lead = Window.partitionBy(\"UserID\").orderBy(\"EnrollDate\")\n",
    "\n",
    "df_next_course = df_enroll.withColumn(\"NextCourse\", lead(\"CourseName\").over(w_lead))\n",
    "df_next_course.select(\"UserID\", \"CourseName\", \"EnrollDate\", \"NextCourse\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2145f6-a485-4e48-9800-1a8e3a388794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n|       CourseName|TotalEnrollments|\n+-----------------+----------------+\n|    Python Basics|               2|\n|Digital Marketing|               1|\n|Excel for Finance|               1|\n+-----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#8 SQL Logic for Dashboard Views\n",
    "\n",
    "# Create SQL views\n",
    "df_enroll.createOrReplaceTempView(\"enrollments\")\n",
    "\n",
    "# View: daily_enrollments\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW daily_enrollments AS\n",
    "    SELECT EnrollDate, COUNT(*) AS TotalEnrollments\n",
    "    FROM enrollments\n",
    "    GROUP BY EnrollDate\n",
    "\"\"\")\n",
    "\n",
    "# View: category_performance\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW category_performance AS\n",
    "    SELECT Category, AVG(Rating) AS AvgRating\n",
    "    FROM enrollments\n",
    "    GROUP BY Category\n",
    "\"\"\")\n",
    "\n",
    "# View: top_3_courses\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW top_3_courses AS\n",
    "    SELECT CourseName, COUNT(*) AS TotalEnrollments\n",
    "    FROM enrollments\n",
    "    GROUP BY CourseName\n",
    "    ORDER BY TotalEnrollments DESC\n",
    "    LIMIT 3\n",
    "\"\"\")\n",
    "\n",
    "# Show top 3 courses\n",
    "spark.sql(\"SELECT * FROM top_3_courses\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a3ee2e5-15e3-4c1d-a1f4-8d521126e8ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|DaysToComplete|IsCompleted|EngagementScore|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|     4|             9|       true|            400|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|    2024-04-18|             45|     3|            16|      false|            135|\n|    E003|  U001|    C003|  ML with PySpark|Data Science|2024-04-03|    2024-05-02|             30|     3|            29|      false|             90|\n|    E004|  U003|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|     5|            16|       true|            500|\n|    E005|  U004|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|     4|            11|       true|            400|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#9 Time Travel and Export Reporting\n",
    "\n",
    "# Time travel: View previous version\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/Workspace/Shared/enrollments_delta\")\n",
    "df_v0.show()\n",
    "\n",
    "# Export to JSON partitioned by Category\n",
    "df_enroll.write.mode(\"overwrite\").partitionBy(\"Category\").json(\"file:/Workspace/Shared/course_json_output\")\n",
    "\n",
    "# Create summary DataFrame and export as Parquet\n",
    "df_summary = df_enroll.groupBy(\"CourseName\").agg(\n",
    "    count(\"*\").alias(\"TotalEnrollments\"),\n",
    "    avg(\"Rating\").alias(\"AvgRating\"),\n",
    "    avg(\"ProgressPercent\").alias(\"AvgProgress\")\n",
    ")\n",
    "\n",
    "df_summary.write.mode(\"overwrite\").parquet(\"file:/Workspace/Shared/course_summary_parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Coding Assessment Set 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}